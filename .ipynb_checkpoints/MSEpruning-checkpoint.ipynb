{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I explore the cuts of the features\n",
    "%matplotlib inline\n",
    "#так тоже можно решить проблему инлайла плотов\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "import _matrixnetapplier as mnet\n",
    "import copy\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import cPickle\n",
    "from StringIO import StringIO\n",
    "n_jobs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#debug purposes\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997 == 9997\n"
     ]
    }
   ],
   "source": [
    "with open('formula/MSLR10k_ef.mx', 'r') as f:\n",
    "    formula = mnet.MatrixnetClassifier(StringIO(cPickle.load(f))) #btw he's a regressor, not classifier\n",
    "\n",
    "depth, nTrees, itr = formula.iterate_trees().next()\n",
    "trees = [tree for tree in itr]\n",
    "print len(trees), '==',nTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_as_h5(path_to_txt,output_name=\"mslr\"):\n",
    "    \n",
    "    print \"opening \"+path_to_txt\n",
    "    f = open(path_to_txt)\n",
    "    labels = []\n",
    "    features = []\n",
    "    print \"extracting...\"\n",
    "    for line in f:\n",
    "        line = line[:line.find('#') - 1]#удалить комменты из конца линии\n",
    "        ls = line.split()\n",
    "        labels.append(int(ls[0]))\n",
    "        features.append([float(x[x.find(':') + 1:]) for x in ls[1:]])\n",
    "    f.close()\n",
    "    print \"converting & sorting...\"\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    features = np.asarray(features)\n",
    "    query = features[:, 0].astype(int)\n",
    "    features = features[:, 1:]\n",
    "    sorter = np.argsort(query)\n",
    "    print \"saving...\"\n",
    "    h5f = h5py.File(output_name, 'w')\n",
    "    h5f.create_dataset('qids', data=query[sorter])\n",
    "    h5f.create_dataset('labels', data=labels[sorter])\n",
    "    h5f.create_dataset('features', data=features[sorter])\n",
    "    h5f.close()\n",
    "    print \"done\"\n",
    "    return features,query,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##warning! this can take a long time. no need to rerun that code if u have CSV files created once.\n",
    "#save_as_h5(\"MSLR10/Fold1/train.txt\",\"mslr_train\")\n",
    "#save_as_h5(\"MSLR10/Fold1/test.txt\",\"mslr_test\")\n",
    "#save_as_h5(\"MSLR10/Fold1/vali.txt\",\"mslr_vali\")\n",
    "#print \"converted that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load training set\n",
    "def load_h5(name):\n",
    "    print \"reading from\",name\n",
    "    h5f = h5py.File(name,'r')\n",
    "    labels = h5f['labels'][:]\n",
    "    qids = h5f['qids'][:]\n",
    "    features = h5f['features'][:]\n",
    "    h5f.close()\n",
    "    print \"done\"\n",
    "    return features, qids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from mslr_train\n",
      "done\n",
      "reading from mslr_test\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "Xtr,Qtr,Ytr = load_h5(\"mslr_train\")\n",
    "Xts,Qts,Yts = load_h5(\"mslr_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from factory import RegressionFactory\n",
    "#s_ind = np.arange(1000)\n",
    "#trainFactory = RegressionFactory(Xtr[s_ind],Ytr[s_ind])\n",
    "#DataFactory is just a data wrapper that can handle splits, predictions, etc. \n",
    "#Used to avoid recomputing metadata at each predictions and passing large argument strings\n",
    "trainFactory = RegressionFactory(Xtr,Ytr)\n",
    "testFactory = RegressionFactory(Xts,Yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (723412L, 136L) qids: 6000\n",
      "test:  (241521L, 136L) qids: 2000\n",
      "qid intersection: 0 (must be 0)\n"
     ]
    }
   ],
   "source": [
    "print \"train: \",Xtr.shape,\"qids:\",len(set(Qtr))\n",
    "print \"test: \",Xts.shape,\"qids:\",len(set(Qts))\n",
    "print \"qid intersection:\",len(set.intersection(set(Qtr),set(Qts))),\"(must be 0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy pruning for the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import greedy\n",
    "from loss_functions import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  650951.041929\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  558858.329132 \n",
      "last loss =  558858.329132\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  506327.910468 \n",
      "last loss =  506327.910468\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  475100.0006 \n",
      "last loss =  475100.0006\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  456614.133348 \n",
      "last loss =  456614.133348\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  444930.002983 \n",
      "last loss =  444930.002983\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  438043.675827 \n",
      "last loss =  438043.675827\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  433368.076071 \n",
      "last loss =  433368.076071\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  430523.388357 \n",
      "last loss =  430523.388357\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  428366.077034 \n",
      "last loss =  428366.077034\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  426694.306363 \n",
      "last loss =  426694.306363\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  425380.395735 \n",
      "last loss =  425380.395735\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  424476.072634 \n",
      "last loss =  424476.072634\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  423496.711868 \n",
      "last loss =  423496.711868\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  422838.338198 \n",
      "last loss =  422838.338198\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  422174.366831 \n",
      "last loss =  422174.366831\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  421579.8668 \n",
      "last loss =  421579.8668\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  421029.666608 \n",
      "last loss =  421029.666608\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  420530.902567 \n",
      "last loss =  420530.902567\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  420000.882858 \n",
      "last loss =  420000.882858\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  419557.451784 \n",
      "last loss =  419557.451784\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  419130.990173 \n",
      "last loss =  419130.990173\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  418647.196106 \n",
      "last loss =  418647.196106\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  418238.546269 \n",
      "last loss =  418238.546269\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  417878.9067 \n",
      "last loss =  417878.9067\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  417563.920217 \n",
      "last loss =  417563.920217\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  417194.695588 \n",
      "last loss =  417194.695588\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  416869.432446 \n",
      "last loss =  416869.432446\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  416546.870169 \n",
      "last loss =  416546.870169\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  416237.917014 \n",
      "last loss =  416237.917014\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  415937.68232 \n",
      "last loss =  415937.68232\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  415662.187669 \n",
      "last loss =  415662.187669\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  415383.466592 \n",
      "last loss =  415383.466592\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  415143.509075 \n",
      "last loss =  415143.509075\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  414864.621237 \n",
      "last loss =  414864.621237\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  414592.014622 \n",
      "last loss =  414592.014622\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  414356.562573 \n",
      "last loss =  414356.562573\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  414112.859473 \n",
      "last loss =  414112.859473\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  413856.167322 \n",
      "last loss =  413856.167322\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  413608.031458 \n",
      "last loss =  413608.031458\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  413360.908928 \n",
      "last loss =  413360.908928\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  413143.412063 \n",
      "last loss =  413143.412063\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  412915.689489 \n",
      "last loss =  412915.689489\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  412666.606352 \n",
      "last loss =  412666.606352\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  412434.864587 \n",
      "last loss =  412434.864587\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  412224.918866 \n",
      "last loss =  412224.918866\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  412030.510982 \n",
      "last loss =  412030.510982\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  411834.805418 \n",
      "last loss =  411834.805418\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  411618.445525 \n",
      "last loss =  411618.445525\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  411401.400291 \n",
      "last loss =  411401.400291\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  411217.132458 \n",
      "last loss =  411217.132458\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  411016.039944 \n",
      "last loss =  411016.039944\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  410812.096245 \n",
      "last loss =  410812.096245\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  410603.438344 \n",
      "last loss =  410603.438344\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  410418.861468 \n",
      "last loss =  410418.861468\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  410231.917546 \n",
      "last loss =  410231.917546\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  410034.091882 \n",
      "last loss =  410034.091882\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  409854.879053 \n",
      "last loss =  409854.879053\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  409680.203992 \n",
      "last loss =  409680.203992\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  409498.877918 \n",
      "last loss =  409498.877918\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  409326.974855 \n",
      "last loss =  409326.974855\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  409142.749227 \n",
      "last loss =  409142.749227\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  408992.90545 \n",
      "last loss =  408992.90545\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  408834.426405 \n",
      "last loss =  408834.426405\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  408657.596476 \n",
      "last loss =  408657.596476\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  408487.506193 \n",
      "last loss =  408487.506193\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  408321.32583 \n",
      "last loss =  408321.32583\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  408146.386275 \n",
      "last loss =  408146.386275\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  407990.828557 \n",
      "last loss =  407990.828557\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  407820.963302 \n",
      "last loss =  407820.963302\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  407642.758809 \n",
      "last loss =  407642.758809\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  407465.735274 \n",
      "last loss =  407465.735274\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  407289.386709 \n",
      "last loss =  407289.386709\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  407138.437571 \n",
      "last loss =  407138.437571\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  406987.372399 \n",
      "last loss =  406987.372399\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  406849.490222 \n",
      "last loss =  406849.490222\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  406683.422559 \n",
      "last loss =  406683.422559\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  406526.456114 \n",
      "last loss =  406526.456114\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  406374.88042 \n",
      "last loss =  406374.88042\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  406234.825073 \n",
      "last loss =  406234.825073\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  406091.68402 \n",
      "last loss =  406091.68402\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  405920.166536 \n",
      "last loss =  405920.166536\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration #"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_greedy = greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                          loss = MSELoss,\n",
    "                                          learning_rate = .25,\n",
    "                                          learning_rate_decay=1.,# no decay\n",
    "                                          nTrees =100,\n",
    "                                          trees_sample_size =200, #chosen from the ensemble at random each iteration\n",
    "                                          verbose = True,\n",
    "                                          regularizer=0.0004, #added to gradient walker's leaf denominator\n",
    "                                          use_joblib=False,\n",
    "                                          n_jobs=-1,\n",
    "                                          joblib_method=\"threads\" #every GIL-ly thing is copied anyways\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate_grid = [0.01,0.05,0.1,0.2]\n",
    "res_grid = [greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                          loss = MSELoss,\n",
    "                                          learning_rate = lrate,\n",
    "                                          learning_rate_decay=1.,# no decay\n",
    "                                          nTrees =100,\n",
    "                                          trees_sample_size =200, #chosen from the ensemble at random each iteration\n",
    "                                          verbose = True,\n",
    "                                          regularizer=0.0004, #added to gradient walker's leaf denominator\n",
    "                                          use_joblib=False,\n",
    "                                          n_jobs=-1,\n",
    "                                          joblib_method=\"threads\" #every GIL-ly thing is copied anyways\n",
    "                                          ) for lrate in lrate_grid]\n",
    "for i in xrange(len(lrate_grid)):\n",
    "    print lrate_grid[i], metrics.mean_squared_error(Yts,y_pred_full,testFactory.predict(res_grid[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_stupid = greedy.predict(testFactory,trees[:200])\n",
    "y_pred_full = greedy.predict(testFactory,trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_greedy = testFactory.predict(res_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print metrics.mean_squared_error(Yts,y_pred_greedy),\n",
    "print metrics.mean_squared_error(Yts,y_pred_stupid),\n",
    "print metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usability distribution\n",
    "thresholds = mnet.get_thresholds(trees,30,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(thresholds)),thresholds[:,2])\n",
    "print sum(thresholds[:,2] >150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get them...\n",
    "thresholds_active = thresholds[thresholds[:,2]>100] #at least 100 times used in the original ensemble\n",
    "print len(thresholds_active)\n",
    "criteria = hierarchy.select_criteria(trainFactory,thresholds_active,4,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = hierarchy.split_upper(trainFactory,criteria,equalizeWeights=False,split_weights=1.,split_inclusion=.7) \n",
    "#при каждом разделении в подвыборку  попадает split_inclusion примеров из другой половины выборки с весом split_weights\n",
    "print [split[i].events.shape[0] for i in split]\n",
    "print [sum(split[i].weights) for i in split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hierarchical stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "#note that it equalizes weights which might be suboptimal.\n",
    "trees_splitted = hierarchy.train_splitted_boosts(trees, trainFactory,criteria,\n",
    "                                                 loss = LogLoss,\n",
    "                                                 learning_rate = 0.25, \n",
    "                                                 nTrees_leaf= 150,\n",
    "                                                 trees_sample_size=200,\n",
    "                                                 regularizer =0.0004,\n",
    "                                                 verbose=True,use_joblib = False,n_jobs = -1,\n",
    "                                                 weights_outside_leaf = 0.75**.25, inclusion_outside_leaf = .5**.25) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_splitted= hierarchy.predict_splitted(testFactory,criteria,trees_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_test = testFactory.weights\n",
    "Yts = testFactory.labels\n",
    "print 'spltd\\t',metrics.mean_squared_error(Yts,y_pred_splitted)\n",
    "print 'greedy\\t',metrics.mean_squared_error(Yts,y_pred_greedy),\n",
    "print 'stupid\\t',metrics.mean_squared_error(Yts,y_pred_stupid),\n",
    "print 'full\\t',metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spltd\t65677.4334464\n",
      "greedy\t"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_greedy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-d5abf254b651>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'spltd\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFactory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_splitted\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m'greedy\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLogLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFactory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_greedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'stupid\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLogLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFactory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_stupid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'full\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLogLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFactory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'new \\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLogLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFactory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_sklearn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_greedy' is not defined"
     ]
    }
   ],
   "source": [
    "print 'spltd\\t', LogLoss.score(testFactory, y_pred_splitted)\n",
    "print 'greedy\\t',LogLoss.score(testFactory, y_pred_greedy)\n",
    "print 'stupid\\t',LogLoss.score(testFactory, y_pred_stupid)\n",
    "print 'full\\t',LogLoss.score(testFactory, y_pred_full)\n",
    "print 'new \\t',LogLoss.score(testFactory, y_pred_sklearn)\n",
    "print \"well...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#AUC learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I know it could have been done without quadratic complexity over trees, but...\n",
    "#btw equalizing 1-0 weights for the training set is not optimal in terms of AUC, but it is close to being so.\n",
    "n_trees =150\n",
    "auc_stupid_lcurve = [(0.5,)]\n",
    "auc_greedy_lcurve = [(0.5,)]\n",
    "auc_splitted_lcurve = [(0.5,)]\n",
    "for i in range(1,n_trees):\n",
    "    #stpd\n",
    "    pred = testFactory.predict(trees[:i])\n",
    "    auc = metrics.roc_auc_score(testFactory.labels,pred,sample_weight=testFactory.weights),\n",
    "    auc_stupid_lcurve.append( auc)    \n",
    "    #grdy\n",
    "    pred = testFactory.predict(res_greedy[:i])#res_greedy_wheel is not optimized for this dissection \n",
    "    auc = metrics.roc_auc_score(testFactory.labels,pred,sample_weight=testFactory.weights),\n",
    "    auc_greedy_lcurve.append( auc)\n",
    "    #split\n",
    "    trees_i = {code:trees_splitted[code][:i] for code in trees_splitted}\n",
    "    pred = hierarchy.predict_splitted(testFactory,criteria,trees_i)\n",
    "\n",
    "    auc = metrics.roc_auc_score(testFactory.labels,pred,sample_weight=testFactory.weights),\n",
    "    auc_splitted_lcurve.append( auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = range(1,n_trees)\n",
    "plt.figure(figsize = [14,14])\n",
    "plt.plot(p,[0.935834322801 for i in range(1,n_trees)],label = \"full\")\n",
    "plt.plot(p,auc_stupid_lcurve[1:n_trees],label = \"stupid\")\n",
    "plt.plot(p,auc_greedy_lcurve[1:n_trees],label = \"greedy\")\n",
    "plt.plot(p,auc_splitted_lcurve[1:n_trees],label = \"splitted\")\n",
    "plt.title('learning curves(AUC)')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LogLoss learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I know it could have been done without quadratic complexity over trees, but...\n",
    "#btw equalizing 1-0 weights for the training set is not optimal in terms of AUC, but it is close to being so.\n",
    "n_trees =150\n",
    "logloss_stupid_lcurve = [(0.5,)]\n",
    "logloss_greedy_lcurve = [(0.5,)]\n",
    "logloss_splitted_lcurve = [(0.5,)]\n",
    "for i in range(1,n_trees):\n",
    "    #stpd\n",
    "    pred = testFactory.predict(trees[:i])\n",
    "    logloss = LogLoss.score(testFactory,pred),\n",
    "    logloss_stupid_lcurve.append( logloss)    \n",
    "    #grdy\n",
    "    pred = testFactory.predict(res_greedy[:i])#res_greedy_wheel is not optimized for this dissection \n",
    "    logloss = LogLoss.score(testFactory,pred),\n",
    "    logloss_greedy_lcurve.append(logloss)\n",
    "    #split\n",
    "    trees_i = {code:trees_splitted[code][:i] for code in trees_splitted}\n",
    "    pred = hierarchy.predict_splitted(testFactory,criteria,trees_i)\n",
    "\n",
    "    logloss = LogLoss.score(testFactory,pred),\n",
    "    logloss_splitted_lcurve.append(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = range(1,n_trees)\n",
    "plt.figure(figsize = [14,14])\n",
    "plt.plot(p,[0.935834322801 for i in range(1,n_trees)],label = \"full\")\n",
    "plt.plot(p,logloss_stupid_lcurve[1:n_trees],label = \"stupid\")\n",
    "plt.plot(p,logloss_lcurve[1:n_trees],label = \"greedy\")\n",
    "plt.plot(p,logloss_splitted_lcurve[1:n_trees],label = \"splitted\")\n",
    "plt.title('learning curves(LogLoss)')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
