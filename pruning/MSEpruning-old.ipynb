{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#math & plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sklearn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#debug purposes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "target_n_trees = 100\n",
    "global_n_jobs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "def cDump(obj,fname):\n",
    "    with open(fname,'w') as f:\n",
    "        cPickle.dump(obj,f)\n",
    "def cLoad(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        return cPickle.load(f)\n",
    "        \n",
    "from StringIO import StringIO\n",
    "import _matrixnetapplier as mnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997 == 9997\n"
     ]
    }
   ],
   "source": [
    "with open('../formula/MSLR10k_ef.mx', 'r') as f:\n",
    "    formula = mnet.MatrixnetClassifier(StringIO(cPickle.load(f))) #btw he's a regressor, not classifier\n",
    "\n",
    "depth, nTrees, itr = formula.iterate_trees().next()\n",
    "trees = [tree for tree in itr]\n",
    "print len(trees), '==',nTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import h5py. H5 IO operations will be unavailable\n"
     ]
    }
   ],
   "source": [
    "import io_ranking as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted that\n",
      "CPU times: user 30 µs, sys: 14 µs, total: 44 µs\n",
      "Wall time: 42.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##warning! this can take a long time. no need to rerun that code if u have CSV files created once.\n",
    "#io.save_csv(\"../data/MSLR10/Fold1/train.txt\",\"../data/MSLR10/mslr_train\")\n",
    "#io.save_csv(\"../data/MSLR10/Fold1/test.txt\",\"../data/MSLR10/mslr_test\")\n",
    "#io.save_csv(\"../data/MSLR10/Fold1/vali.txt\",\"../data/MSLR10/mslr_vali\")\n",
    "print \"converted that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from ../data/MSLR10/mslr_train\n",
      "done\n",
      "reading from ../data/MSLR10/mslr_test\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load training set\n",
    "Xtr,Qtr,Ytr = io.load_csv(\"../data/MSLR10/mslr_train\")\n",
    "Xts,Qts,Yts = io.load_csv(\"../data/MSLR10/mslr_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from factory import RegressionFactory\n",
    "#DataFactory is just a data wrapper that can handle splits, predictions, etc. \n",
    "#Used to avoid recomputing metadata at each predictions and passing large argument strings\n",
    "trainFactory = RegressionFactory(Xtr,Ytr)\n",
    "testFactory = RegressionFactory(Xts,Yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xtr,Ytr,Xts,Yts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainFactory.labels = trainFactory.labels.astype('int8')\n",
    "trainFactory.events = trainFactory.events.astype('float32')\n",
    "trainFactory.weights = trainFactory.weights.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (723412, 136) qids: 6000\n",
      "test:  (241521, 136) qids: 2000\n",
      "qid intersection: 0 (must be 0)\n"
     ]
    }
   ],
   "source": [
    "print \"train: \",trainFactory.events.shape,\"qids:\",len(set(Qtr))\n",
    "print \"test: \",testFactory.events.shape,\"qids:\",len(set(Qts))\n",
    "print \"qid intersection:\",len(set.intersection(set(Qtr),set(Qts))),\"(must be 0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy pruning for the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import greedy\n",
    "from loss_functions import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  463018.540185\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  444651.715193 \n",
      "last loss =  444651.715193\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  434686.18885 \n",
      "last loss =  434686.18885\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  428073.844285 \n",
      "last loss =  428073.844285\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  423237.158698 \n",
      "last loss =  423237.158698\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  420240.959481 \n",
      "last loss =  420240.959481\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  417492.989411 \n",
      "last loss =  417492.989411\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  415322.558968 \n",
      "last loss =  415322.558968\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  413494.099163 \n",
      "last loss =  413494.099163\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  411999.270369 \n",
      "last loss =  411999.270369\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  410772.911795 \n",
      "last loss =  410772.911795\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  409702.939778 \n",
      "last loss =  409702.939778\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  408758.771019 \n",
      "last loss =  408758.771019\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  407959.487193 \n",
      "last loss =  407959.487193\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  407218.177582 \n",
      "last loss =  407218.177582\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  406512.675979 \n",
      "last loss =  406512.675979\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  405851.575218 \n",
      "last loss =  405851.575218\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  405218.093157 \n",
      "last loss =  405218.093157\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  404577.19202 \n",
      "last loss =  404577.19202\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  403981.109093 \n",
      "last loss =  403981.109093\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  403505.742411 \n",
      "last loss =  403505.742411\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  402940.905687 \n",
      "last loss =  402940.905687\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  402454.158625 \n",
      "last loss =  402454.158625\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  402043.485587 \n",
      "last loss =  402043.485587\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  401588.581096 \n",
      "last loss =  401588.581096\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  401161.679642 \n",
      "last loss =  401161.679642\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  400788.337668 \n",
      "last loss =  400788.337668\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  400409.696023 \n",
      "last loss =  400409.696023\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  400020.75915 \n",
      "last loss =  400020.75915\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  399665.377928 \n",
      "last loss =  399665.377928\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  399331.511162 \n",
      "last loss =  399331.511162\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  399006.300073 \n",
      "last loss =  399006.300073\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  398693.359652 \n",
      "last loss =  398693.359652\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  398371.648196 \n",
      "last loss =  398371.648196\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  398081.124747 \n",
      "last loss =  398081.124747\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  397805.984006 \n",
      "last loss =  397805.984006\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  397487.427709 \n",
      "last loss =  397487.427709\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  397215.653276 \n",
      "last loss =  397215.653276\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  396945.133653 \n",
      "last loss =  396945.133653\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  396683.207047 \n",
      "last loss =  396683.207047\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  396421.228643 \n",
      "last loss =  396421.228643\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  396173.289 \n",
      "last loss =  396173.289\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  395924.773445 \n",
      "last loss =  395924.773445\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  395689.776007 \n",
      "last loss =  395689.776007\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  395424.923381 \n",
      "last loss =  395424.923381\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  395198.509051 \n",
      "last loss =  395198.509051\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  394967.617914 \n",
      "last loss =  394967.617914\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  394755.17244 \n",
      "last loss =  394755.17244\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  394529.287718 \n",
      "last loss =  394529.287718\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  394305.422225 \n",
      "last loss =  394305.422225\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  394076.206969 \n",
      "last loss =  394076.206969\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  393857.92639 \n",
      "last loss =  393857.92639\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  393643.550395 \n",
      "last loss =  393643.550395\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  393442.039118 \n",
      "last loss =  393442.039118\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  393226.445792 \n",
      "last loss =  393226.445792\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  393020.376933 \n",
      "last loss =  393020.376933\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  392809.519656 \n",
      "last loss =  392809.519656\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  392603.730759 \n",
      "last loss =  392603.730759\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  392420.69073 \n",
      "last loss =  392420.69073\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  392211.956565 \n",
      "last loss =  392211.956565\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  392031.261958 \n",
      "last loss =  392031.261958\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  391854.549302 \n",
      "last loss =  391854.549302\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  391653.239001 \n",
      "last loss =  391653.239001\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  391467.295494 \n",
      "last loss =  391467.295494\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  391300.793281 \n",
      "last loss =  391300.793281\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  391118.215191 \n",
      "last loss =  391118.215191\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  390935.128179 \n",
      "last loss =  390935.128179\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  390752.406805 \n",
      "last loss =  390752.406805\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  390583.330936 \n",
      "last loss =  390583.330936\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  390410.124013 \n",
      "last loss =  390410.124013\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  390241.386893 \n",
      "last loss =  390241.386893\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  390075.831683 \n",
      "last loss =  390075.831683\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  389913.386026 \n",
      "last loss =  389913.386026\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  389751.343894 \n",
      "last loss =  389751.343894\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  389583.25583 \n",
      "last loss =  389583.25583\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  389422.194213 \n",
      "last loss =  389422.194213\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  389272.64374 \n",
      "last loss =  389272.64374\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  389117.793714 \n",
      "last loss =  389117.793714\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  388958.473289 \n",
      "last loss =  388958.473289\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  388809.272143 \n",
      "last loss =  388809.272143\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  388662.146094 \n",
      "last loss =  388662.146094\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  388511.673407 \n",
      "last loss =  388511.673407\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  388360.944709 \n",
      "last loss =  388360.944709\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  388212.41355 \n",
      "last loss =  388212.41355\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  388073.923396 \n",
      "last loss =  388073.923396\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  387933.137921 \n",
      "last loss =  387933.137921\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  387781.42507 \n",
      "last loss =  387781.42507\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  387630.197913 \n",
      "last loss =  387630.197913\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  387492.692251 \n",
      "last loss =  387492.692251\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  387353.259636 \n",
      "last loss =  387353.259636\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  387213.920206 \n",
      "last loss =  387213.920206\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  387077.481452 \n",
      "last loss =  387077.481452\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  386947.494341 \n",
      "last loss =  386947.494341\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  386821.523441 \n",
      "last loss =  386821.523441\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  386690.612762 \n",
      "last loss =  386690.612762\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  386558.704341 \n",
      "last loss =  386558.704341\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  386433.20474 \n",
      "last loss =  386433.20474\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  386299.585746 \n",
      "last loss =  386299.585746\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  386170.537118 \n",
      "last loss =  386170.537118\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  386045.054111 \n",
      "last loss =  386045.054111\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "CPU times: user 47min 24s, sys: 4min 51s, total: 52min 15s\n",
      "Wall time: 10min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "read = True\n",
    "fname=\"../dumps/last_greedy.pcl\"\n",
    "if not read:\n",
    "    trees_greedy = greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                              loss = MSELoss,\n",
    "                                              learning_rate = .35,\n",
    "                                              learning_rate_decay=1.,# no decay\n",
    "                                              nTrees = target_n_trees,\n",
    "                                              trees_sample_size =500, #chosen from the ensemble at random each iteration\n",
    "                                              verbose = True,\n",
    "                                              regularizer=0.0005*len(trainFactory.labels), #added to gradient walker's leaf denominator\n",
    "                                              use_joblib=True,\n",
    "                                              n_jobs=global_n_jobs, #threading by default\n",
    "                                              )\n",
    "    cDump(trees_greedy,fname)\n",
    "else:\n",
    "    trees_greedy = cLoad(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from greedy import PrunedFormula as pf\n",
    "bias = 0.0# NOT np.average(trainFactory.labels)\n",
    "trees_stupid = pf(trees[:target_n_trees],bias)\n",
    "trees_full = pf(trees,bias)\n",
    "\n",
    "y_pred_stupid = trees_stupid.predict(testFactory)\n",
    "y_pred_full = trees_full.predict(testFactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_greedy = trees_greedy.predict(testFactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565785832991 0.695731252371 0.554568902063\n",
      "well...\n"
     ]
    }
   ],
   "source": [
    "print metrics.mean_squared_error(testFactory.labels,y_pred_greedy),\n",
    "print metrics.mean_squared_error(testFactory.labels,y_pred_stupid),\n",
    "print metrics.mean_squared_error(testFactory.labels,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723412\n"
     ]
    }
   ],
   "source": [
    "print len(trainFactory.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  459613.857508\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  442190.186901 \n",
      "last loss =  442190.186901\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  432770.493453 \n",
      "last loss =  432770.493453\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  426551.32343 \n",
      "last loss =  426551.32343\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  421438.664031 \n",
      "last loss =  421438.664031\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  418032.44535 \n",
      "last loss =  418032.44535\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  415542.623874 \n",
      "last loss =  415542.623874\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  413525.319562 \n",
      "last loss =  413525.319562\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  411799.900479 \n",
      "last loss =  411799.900479\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  410458.954781 \n",
      "last loss =  410458.954781\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  409164.290675 \n",
      "last loss =  409164.290675\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  407881.055985 \n",
      "last loss =  407881.055985\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  406723.978116 \n",
      "last loss =  406723.978116\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  405829.857626 \n",
      "last loss =  405829.857626\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  405047.91824 \n",
      "last loss =  405047.91824\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  404235.770227 \n",
      "last loss =  404235.770227\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  403502.09108 \n",
      "last loss =  403502.09108\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  402832.258444 \n",
      "last loss =  402832.258444\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  402208.927926 \n",
      "last loss =  402208.927926\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  401601.532601 \n",
      "last loss =  401601.532601\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  401092.583421 \n",
      "last loss =  401092.583421\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  400548.391343 \n",
      "last loss =  400548.391343\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  400053.76947 \n",
      "last loss =  400053.76947\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  399553.677484 \n",
      "last loss =  399553.677484\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  399031.521995 \n",
      "last loss =  399031.521995\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  398567.306363 \n",
      "last loss =  398567.306363\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  398134.934728 \n",
      "last loss =  398134.934728\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  397754.704677 \n",
      "last loss =  397754.704677\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  397321.288448 \n",
      "last loss =  397321.288448\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  396962.15264 \n",
      "last loss =  396962.15264\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  396607.172616 \n",
      "last loss =  396607.172616\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  396249.316183 \n",
      "last loss =  396249.316183\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  395907.958094 \n",
      "last loss =  395907.958094\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  395583.92785 \n",
      "last loss =  395583.92785\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  395212.856154 \n",
      "last loss =  395212.856154\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  394877.678308 \n",
      "last loss =  394877.678308\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  394556.337545 \n",
      "last loss =  394556.337545\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  394218.796919 \n",
      "last loss =  394218.796919\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  393927.158971 \n",
      "last loss =  393927.158971\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  393621.562505 \n",
      "last loss =  393621.562505\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  393321.825252 \n",
      "last loss =  393321.825252\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  393018.231535 \n",
      "last loss =  393018.231535\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  392698.179893 \n",
      "last loss =  392698.179893\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  392412.331341 \n",
      "last loss =  392412.331341\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  392137.198141 \n",
      "last loss =  392137.198141\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  391853.344335 \n",
      "last loss =  391853.344335\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  391580.652284 \n",
      "last loss =  391580.652284\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  391292.47453 \n",
      "last loss =  391292.47453\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  391025.139104 \n",
      "last loss =  391025.139104\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  390764.091025 \n",
      "last loss =  390764.091025\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  390492.874439 \n",
      "last loss =  390492.874439\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  390246.679286 \n",
      "last loss =  390246.679286\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  390004.436919 \n",
      "last loss =  390004.436919\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  389750.284563 \n",
      "last loss =  389750.284563\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  389508.326911 \n",
      "last loss =  389508.326911\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  389258.598173 \n",
      "last loss =  389258.598173\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  389022.760189 \n",
      "last loss =  389022.760189\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  388799.720567 \n",
      "last loss =  388799.720567\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  388565.8824 \n",
      "last loss =  388565.8824\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  388319.764344 \n",
      "last loss =  388319.764344\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  388084.305245 \n",
      "last loss =  388084.305245\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  387878.369159 \n",
      "last loss =  387878.369159\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  387679.465042 \n",
      "last loss =  387679.465042\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  387472.78435 \n",
      "last loss =  387472.78435\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  387256.4729 \n",
      "last loss =  387256.4729\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  387069.297348 \n",
      "last loss =  387069.297348\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  386879.618483 \n",
      "last loss =  386879.618483\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  386666.486269 \n",
      "last loss =  386666.486269\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  386475.279135 \n",
      "last loss =  386475.279135\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  386284.463534 \n",
      "last loss =  386284.463534\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  386086.310797 \n",
      "last loss =  386086.310797\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  385898.14283 \n",
      "last loss =  385898.14283\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  385708.081909 \n",
      "last loss =  385708.081909\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  385527.285558 \n",
      "last loss =  385527.285558\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  385322.22889 \n",
      "last loss =  385322.22889\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  385140.737181 \n",
      "last loss =  385140.737181\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  384950.756145 \n",
      "last loss =  384950.756145\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  384776.011862 \n",
      "last loss =  384776.011862\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  384578.672349 \n",
      "last loss =  384578.672349\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  384399.344633 \n",
      "last loss =  384399.344633\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  384236.060988 \n",
      "last loss =  384236.060988\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  384065.822994 \n",
      "last loss =  384065.822994\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  383887.335646 \n",
      "last loss =  383887.335646\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  383718.170935 \n",
      "last loss =  383718.170935\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  383538.23024 \n",
      "last loss =  383538.23024\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  383364.327434 \n",
      "last loss =  383364.327434\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  383202.044326 \n",
      "last loss =  383202.044326\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  383034.431384 \n",
      "last loss =  383034.431384\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  382875.055016 \n",
      "last loss =  382875.055016\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  382708.825839 \n",
      "last loss =  382708.825839\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  382529.439109 \n",
      "last loss =  382529.439109\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  382365.7041 \n",
      "last loss =  382365.7041\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  382208.112695 \n",
      "last loss =  382208.112695\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  382049.620285 \n",
      "last loss =  382049.620285\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  381908.228993 \n",
      "last loss =  381908.228993\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  381750.706375 \n",
      "last loss =  381750.706375\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  381599.66979 \n",
      "last loss =  381599.66979\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  381451.599874 \n",
      "last loss =  381451.599874\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  381301.626641 \n",
      "last loss =  381301.626641\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  381154.115778 \n",
      "last loss =  381154.115778\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  461419.181166\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  445085.621693 \n",
      "last loss =  445085.621693\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  435556.285446 \n",
      "last loss =  435556.285446\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  428826.929649 \n",
      "last loss =  428826.929649\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  424330.329572 \n",
      "last loss =  424330.329572\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  421036.926539 \n",
      "last loss =  421036.926539\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  418335.72466 \n",
      "last loss =  418335.72466\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  416225.350496 \n",
      "last loss =  416225.350496\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  414611.083671 \n",
      "last loss =  414611.083671\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  413011.722359 \n",
      "last loss =  413011.722359\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  411721.374342 \n",
      "last loss =  411721.374342\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  410737.79895 \n",
      "last loss =  410737.79895\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  409770.375903 \n",
      "last loss =  409770.375903\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  408944.559348 \n",
      "last loss =  408944.559348\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  408211.383269 \n",
      "last loss =  408211.383269\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  407471.825117 \n",
      "last loss =  407471.825117\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  406821.223006 \n",
      "last loss =  406821.223006\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  406177.140829 \n",
      "last loss =  406177.140829\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  405593.317125 \n",
      "last loss =  405593.317125\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  405037.171084 \n",
      "last loss =  405037.171084\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  404538.150467 \n",
      "last loss =  404538.150467\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  404032.009736 \n",
      "last loss =  404032.009736\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  403560.219966 \n",
      "last loss =  403560.219966\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  403145.348084 \n",
      "last loss =  403145.348084\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  402698.370521 \n",
      "last loss =  402698.370521\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  402297.895048 \n",
      "last loss =  402297.895048\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  401884.475725 \n",
      "last loss =  401884.475725\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  401472.715595 \n",
      "last loss =  401472.715595\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  401125.387555 \n",
      "last loss =  401125.387555\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  400769.179931 \n",
      "last loss =  400769.179931\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  400439.943583 \n",
      "last loss =  400439.943583\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  400137.693406 \n",
      "last loss =  400137.693406\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  399828.213742 \n",
      "last loss =  399828.213742\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  399528.314554 \n",
      "last loss =  399528.314554\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  399244.396687 \n",
      "last loss =  399244.396687\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  398955.500135 \n",
      "last loss =  398955.500135\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  398657.483818 \n",
      "last loss =  398657.483818\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  398401.625539 \n",
      "last loss =  398401.625539\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  398110.711836 \n",
      "last loss =  398110.711836\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  397837.922919 \n",
      "last loss =  397837.922919\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  397597.283356 \n",
      "last loss =  397597.283356\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  397354.413552 \n",
      "last loss =  397354.413552\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  397085.210025 \n",
      "last loss =  397085.210025\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  396835.609949 \n",
      "last loss =  396835.609949\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  396594.263835 \n",
      "last loss =  396594.263835\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  396356.749441 \n",
      "last loss =  396356.749441\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  396082.912969 \n",
      "last loss =  396082.912969\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  395854.22525 \n",
      "last loss =  395854.22525\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  395639.993352 \n",
      "last loss =  395639.993352\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  395422.061766 \n",
      "last loss =  395422.061766\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  395217.804698 \n",
      "last loss =  395217.804698\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  395012.227017 \n",
      "last loss =  395012.227017\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  394801.741842 \n",
      "last loss =  394801.741842\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  394589.684349 \n",
      "last loss =  394589.684349\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  394412.261537 \n",
      "last loss =  394412.261537\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  394227.001504 \n",
      "last loss =  394227.001504\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  394033.392634 \n",
      "last loss =  394033.392634\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  393839.333513 \n",
      "last loss =  393839.333513\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  393656.336676 \n",
      "last loss =  393656.336676\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  393478.40091 \n",
      "last loss =  393478.40091\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  393312.589559 \n",
      "last loss =  393312.589559\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  393145.361434 \n",
      "last loss =  393145.361434\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  392954.506201 \n",
      "last loss =  392954.506201\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  392776.001198 \n",
      "last loss =  392776.001198\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  392579.953016 \n",
      "last loss =  392579.953016\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  392402.842293 \n",
      "last loss =  392402.842293\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  392235.279124 \n",
      "last loss =  392235.279124\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  392057.883169 \n",
      "last loss =  392057.883169\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  391896.606234 \n",
      "last loss =  391896.606234\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  391738.600363 \n",
      "last loss =  391738.600363\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  391560.457008 \n",
      "last loss =  391560.457008\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  391399.664633 \n",
      "last loss =  391399.664633\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  391243.856369 \n",
      "last loss =  391243.856369\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  391081.945302 \n",
      "last loss =  391081.945302\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  390916.336511 \n",
      "last loss =  390916.336511\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  390759.90305 \n",
      "last loss =  390759.90305\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  390617.877612 \n",
      "last loss =  390617.877612\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  390464.94733 \n",
      "last loss =  390464.94733\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  390307.274708 \n",
      "last loss =  390307.274708\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  390163.393068 \n",
      "last loss =  390163.393068\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  390016.351095 \n",
      "last loss =  390016.351095\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  389881.432643 \n",
      "last loss =  389881.432643\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  389737.402638 \n",
      "last loss =  389737.402638\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  389596.937111 \n",
      "last loss =  389596.937111\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  389465.84047 \n",
      "last loss =  389465.84047\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  389329.01681 \n",
      "last loss =  389329.01681\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  389190.399638 \n",
      "last loss =  389190.399638\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  389057.679127 \n",
      "last loss =  389057.679127\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  388937.005986 \n",
      "last loss =  388937.005986\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  388803.059223 \n",
      "last loss =  388803.059223\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  388671.278774 \n",
      "last loss =  388671.278774\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  388539.523891 \n",
      "last loss =  388539.523891\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  388404.674631 \n",
      "last loss =  388404.674631\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  388262.591632 \n",
      "last loss =  388262.591632\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  388139.62779 \n",
      "last loss =  388139.62779\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  388007.743142 \n",
      "last loss =  388007.743142\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  387880.238963 \n",
      "last loss =  387880.238963\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  387761.257714 \n",
      "last loss =  387761.257714\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  387638.288685 \n",
      "last loss =  387638.288685\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  387519.487729 \n",
      "last loss =  387519.487729\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  455950.013119\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  439561.991364 \n",
      "last loss =  439561.991364\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  429726.301622 \n",
      "last loss =  429726.301622\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  423976.41885 \n",
      "last loss =  423976.41885\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  419630.567385 \n",
      "last loss =  419630.567385\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  416152.89 \n",
      "last loss =  416152.89\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  413321.617262 \n",
      "last loss =  413321.617262\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  411613.916074 \n",
      "last loss =  411613.916074\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  410055.794649 \n",
      "last loss =  410055.794649\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  408847.407385 \n",
      "last loss =  408847.407385\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  407746.754675 \n",
      "last loss =  407746.754675\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  406701.212803 \n",
      "last loss =  406701.212803\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  405709.059209 \n",
      "last loss =  405709.059209\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  404780.912873 \n",
      "last loss =  404780.912873\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  403977.415192 \n",
      "last loss =  403977.415192\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  403215.089671 \n",
      "last loss =  403215.089671\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  402584.901079 \n",
      "last loss =  402584.901079\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  401924.74414 \n",
      "last loss =  401924.74414\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  401224.667766 \n",
      "last loss =  401224.667766\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  400644.183267 \n",
      "last loss =  400644.183267\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  400091.876951 \n",
      "last loss =  400091.876951\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  399592.931102 \n",
      "last loss =  399592.931102\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  399029.241218 \n",
      "last loss =  399029.241218\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  398522.376743 \n",
      "last loss =  398522.376743\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  398060.393349 \n",
      "last loss =  398060.393349\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  397628.896581 \n",
      "last loss =  397628.896581\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  397200.682357 \n",
      "last loss =  397200.682357\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  396780.347088 \n",
      "last loss =  396780.347088\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  396364.678545 \n",
      "last loss =  396364.678545\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  395991.969021 \n",
      "last loss =  395991.969021\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  395605.344629 \n",
      "last loss =  395605.344629\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  395234.25644 \n",
      "last loss =  395234.25644\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  394874.846644 \n",
      "last loss =  394874.846644\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  394551.618506 \n",
      "last loss =  394551.618506\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  394205.263284 \n",
      "last loss =  394205.263284\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  393875.072866 \n",
      "last loss =  393875.072866\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  393559.790943 \n",
      "last loss =  393559.790943\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  393232.273375 \n",
      "last loss =  393232.273375\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  392917.627069 \n",
      "last loss =  392917.627069\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  392579.078697 \n",
      "last loss =  392579.078697\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  392258.575991 \n",
      "last loss =  392258.575991\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  391939.985956 \n",
      "last loss =  391939.985956\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  391642.135271 \n",
      "last loss =  391642.135271\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  391363.829633 \n",
      "last loss =  391363.829633\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  391076.587795 \n",
      "last loss =  391076.587795\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  390800.099935 \n",
      "last loss =  390800.099935\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  390523.576208 \n",
      "last loss =  390523.576208\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  390235.872949 \n",
      "last loss =  390235.872949\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  389964.482811 \n",
      "last loss =  389964.482811\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  389712.679641 \n",
      "last loss =  389712.679641\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  389461.451546 \n",
      "last loss =  389461.451546\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  389217.568696 \n",
      "last loss =  389217.568696\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  388956.223353 \n",
      "last loss =  388956.223353\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  388717.61487 \n",
      "last loss =  388717.61487\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  388471.624986 \n",
      "last loss =  388471.624986\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  388224.652368 \n",
      "last loss =  388224.652368\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  387998.107662 \n",
      "last loss =  387998.107662\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  387776.156585 \n",
      "last loss =  387776.156585\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  387504.728409 \n",
      "last loss =  387504.728409\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  387277.121857 \n",
      "last loss =  387277.121857\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  387058.265146 \n",
      "last loss =  387058.265146\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  386848.303709 \n",
      "last loss =  386848.303709\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  386632.085309 \n",
      "last loss =  386632.085309\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  386408.485673 \n",
      "last loss =  386408.485673\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  386191.87419 \n",
      "last loss =  386191.87419\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  385979.181157 \n",
      "last loss =  385979.181157\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  385777.25245 \n",
      "last loss =  385777.25245\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  385570.127286 \n",
      "last loss =  385570.127286\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  385373.796505 \n",
      "last loss =  385373.796505\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  385158.274446 \n",
      "last loss =  385158.274446\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  384945.187408 \n",
      "last loss =  384945.187408\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  384740.874277 \n",
      "last loss =  384740.874277\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  384548.768499 \n",
      "last loss =  384548.768499\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  384361.475069 \n",
      "last loss =  384361.475069\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  384170.850288 \n",
      "last loss =  384170.850288\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  383968.55384 \n",
      "last loss =  383968.55384\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  383789.311516 \n",
      "last loss =  383789.311516\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  383611.379919 \n",
      "last loss =  383611.379919\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  383426.504926 \n",
      "last loss =  383426.504926\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  383241.285917 \n",
      "last loss =  383241.285917\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  383050.986516 \n",
      "last loss =  383050.986516\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  382881.096342 \n",
      "last loss =  382881.096342\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  382712.591514 \n",
      "last loss =  382712.591514\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  382524.519549 \n",
      "last loss =  382524.519549\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  382357.540591 \n",
      "last loss =  382357.540591\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  382193.715899 \n",
      "last loss =  382193.715899\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  382025.257564 \n",
      "last loss =  382025.257564\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  381865.353336 \n",
      "last loss =  381865.353336\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  381713.323038 \n",
      "last loss =  381713.323038\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  381559.628871 \n",
      "last loss =  381559.628871\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  381404.394569 \n",
      "last loss =  381404.394569\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  381245.342953 \n",
      "last loss =  381245.342953\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  381061.495933 \n",
      "last loss =  381061.495933\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  380905.037352 \n",
      "last loss =  380905.037352\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  380753.707798 \n",
      "last loss =  380753.707798\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  380576.113321 \n",
      "last loss =  380576.113321\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  380417.658835 \n",
      "last loss =  380417.658835\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  380253.23726 \n",
      "last loss =  380253.23726\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  380075.940957 \n",
      "last loss =  380075.940957\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  379918.242989 \n",
      "last loss =  379918.242989\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  459108.72303\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  442743.963814 \n",
      "last loss =  442743.963814\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  433410.422988 \n",
      "last loss =  433410.422988\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  426346.643761 \n",
      "last loss =  426346.643761\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  422078.193599 \n",
      "last loss =  422078.193599\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  418903.808162 \n",
      "last loss =  418903.808162\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  416318.089199 \n",
      "last loss =  416318.089199\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  414250.789128 \n",
      "last loss =  414250.789128\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  412651.674585 \n",
      "last loss =  412651.674585\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  411304.216802 \n",
      "last loss =  411304.216802\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  409891.219861 \n",
      "last loss =  409891.219861\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  408902.23145 \n",
      "last loss =  408902.23145\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  407920.264825 \n",
      "last loss =  407920.264825\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  407085.51166 \n",
      "last loss =  407085.51166\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  406289.477036 \n",
      "last loss =  406289.477036\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  405561.590416 \n",
      "last loss =  405561.590416\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  404926.869506 \n",
      "last loss =  404926.869506\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  404360.549673 \n",
      "last loss =  404360.549673\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  403741.15716 \n",
      "last loss =  403741.15716\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  403154.327318 \n",
      "last loss =  403154.327318\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  402616.618039 \n",
      "last loss =  402616.618039\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  402128.522487 \n",
      "last loss =  402128.522487\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  401670.863528 \n",
      "last loss =  401670.863528\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  401257.645233 \n",
      "last loss =  401257.645233\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  400846.238161 \n",
      "last loss =  400846.238161\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  400458.440777 \n",
      "last loss =  400458.440777\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  400069.724275 \n",
      "last loss =  400069.724275\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  399716.361222 \n",
      "last loss =  399716.361222\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  399337.171147 \n",
      "last loss =  399337.171147\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  398986.205149 \n",
      "last loss =  398986.205149\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  398684.973648 \n",
      "last loss =  398684.973648\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  398374.969144 \n",
      "last loss =  398374.969144\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  398077.621718 \n",
      "last loss =  398077.621718\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  397808.735007 \n",
      "last loss =  397808.735007\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  397529.591408 \n",
      "last loss =  397529.591408\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  397251.674737 \n",
      "last loss =  397251.674737\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  396978.929624 \n",
      "last loss =  396978.929624\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  396727.49894 \n",
      "last loss =  396727.49894\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  396468.012539 \n",
      "last loss =  396468.012539\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  396211.223174 \n",
      "last loss =  396211.223174\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  395950.496051 \n",
      "last loss =  395950.496051\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  395680.092096 \n",
      "last loss =  395680.092096\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  395429.469815 \n",
      "last loss =  395429.469815\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  395197.273745 \n",
      "last loss =  395197.273745\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  394934.922213 \n",
      "last loss =  394934.922213\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  394711.262433 \n",
      "last loss =  394711.262433\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  394484.032791 \n",
      "last loss =  394484.032791\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  394263.919945 \n",
      "last loss =  394263.919945\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  394048.700862 \n",
      "last loss =  394048.700862\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  393840.776474 \n",
      "last loss =  393840.776474\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  393621.871253 \n",
      "last loss =  393621.871253\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  393417.718415 \n",
      "last loss =  393417.718415\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  393223.319806 \n",
      "last loss =  393223.319806\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  393024.162749 \n",
      "last loss =  393024.162749\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  392819.23557 \n",
      "last loss =  392819.23557\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  392612.396391 \n",
      "last loss =  392612.396391\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  392417.886322 \n",
      "last loss =  392417.886322\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  392237.801283 \n",
      "last loss =  392237.801283\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  392034.327523 \n",
      "last loss =  392034.327523\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  391833.002702 \n",
      "last loss =  391833.002702\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  391653.166503 \n",
      "last loss =  391653.166503\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  391479.95459 \n",
      "last loss =  391479.95459\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  391319.921725 \n",
      "last loss =  391319.921725\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  391156.003635 \n",
      "last loss =  391156.003635\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  390985.099264 \n",
      "last loss =  390985.099264\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  390807.670127 \n",
      "last loss =  390807.670127\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  390635.991723 \n",
      "last loss =  390635.991723\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  390475.960893 \n",
      "last loss =  390475.960893\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  390320.635707 \n",
      "last loss =  390320.635707\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  390163.999386 \n",
      "last loss =  390163.999386\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  389995.950913 \n",
      "last loss =  389995.950913\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  389845.859609 \n",
      "last loss =  389845.859609\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  389674.020306 \n",
      "last loss =  389674.020306\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  389518.661282 \n",
      "last loss =  389518.661282\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  389362.440417 \n",
      "last loss =  389362.440417\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  389214.959482 \n",
      "last loss =  389214.959482\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  389068.443204 \n",
      "last loss =  389068.443204\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  388928.892163 \n",
      "last loss =  388928.892163\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  388786.979149 \n",
      "last loss =  388786.979149\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  388641.086882 \n",
      "last loss =  388641.086882\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  388484.646549 \n",
      "last loss =  388484.646549\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  388337.473509 \n",
      "last loss =  388337.473509\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  388196.731569 \n",
      "last loss =  388196.731569\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  388063.136596 \n",
      "last loss =  388063.136596\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  387927.964235 \n",
      "last loss =  387927.964235\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  387793.463046 \n",
      "last loss =  387793.463046\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  387668.291204 \n",
      "last loss =  387668.291204\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  387534.058316 \n",
      "last loss =  387534.058316\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  387409.272133 \n",
      "last loss =  387409.272133\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  387273.477251 \n",
      "last loss =  387273.477251\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  387153.699703 \n",
      "last loss =  387153.699703\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  387008.881508 \n",
      "last loss =  387008.881508\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  386883.80508 \n",
      "last loss =  386883.80508\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  386750.998103 \n",
      "last loss =  386750.998103\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  386608.539393 \n",
      "last loss =  386608.539393\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  386476.584466 \n",
      "last loss =  386476.584466\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  386354.251096 \n",
      "last loss =  386354.251096\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  386232.569712 \n",
      "last loss =  386232.569712\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  386106.515218 \n",
      "last loss =  386106.515218\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  385984.108491 \n",
      "last loss =  385984.108491\n",
      "learning_rate =  0.4\n",
      "sample_size 500\n",
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  448924.277188\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  432782.353596 \n",
      "last loss =  432782.353596\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  424595.773079 \n",
      "last loss =  424595.773079\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  418849.865491 \n",
      "last loss =  418849.865491\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  415963.335805 \n",
      "last loss =  415963.335805\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  413655.868201 \n",
      "last loss =  413655.868201\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  411761.873893 \n",
      "last loss =  411761.873893\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  410124.830696 \n",
      "last loss =  410124.830696\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  408712.73234 \n",
      "last loss =  408712.73234\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  407529.095214 \n",
      "last loss =  407529.095214\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  406430.005016 \n",
      "last loss =  406430.005016\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  405515.582035 \n",
      "last loss =  405515.582035\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  404658.340205 \n",
      "last loss =  404658.340205\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  403868.355097 \n",
      "last loss =  403868.355097\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  403077.644105 \n",
      "last loss =  403077.644105\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  402353.875479 \n",
      "last loss =  402353.875479\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  401708.283944 \n",
      "last loss =  401708.283944\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  400983.452326 \n",
      "last loss =  400983.452326\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  400425.688792 \n",
      "last loss =  400425.688792\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  399785.563786 \n",
      "last loss =  399785.563786\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  399297.911486 \n",
      "last loss =  399297.911486\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  398793.092044 \n",
      "last loss =  398793.092044\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  398298.67077 \n",
      "last loss =  398298.67077\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  397829.379374 \n",
      "last loss =  397829.379374\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  397304.58152 \n",
      "last loss =  397304.58152\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  396851.039612 \n",
      "last loss =  396851.039612\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  396438.988067 \n",
      "last loss =  396438.988067\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  396026.738952 \n",
      "last loss =  396026.738952\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  395639.427278 \n",
      "last loss =  395639.427278\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  395206.567683 \n",
      "last loss =  395206.567683\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  394812.342061 \n",
      "last loss =  394812.342061\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  394371.618665 \n",
      "last loss =  394371.618665\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  393970.566735 \n",
      "last loss =  393970.566735\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  393606.449703 \n",
      "last loss =  393606.449703\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  393193.820738 \n",
      "last loss =  393193.820738\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  392833.867358 \n",
      "last loss =  392833.867358\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  392497.607549 \n",
      "last loss =  392497.607549\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  392137.597805 \n",
      "last loss =  392137.597805\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  391799.935744 \n",
      "last loss =  391799.935744\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  391431.730517 \n",
      "last loss =  391431.730517\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  391086.754242 \n",
      "last loss =  391086.754242\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  390703.876631 \n",
      "last loss =  390703.876631\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  390394.234873 \n",
      "last loss =  390394.234873\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  390091.203139 \n",
      "last loss =  390091.203139\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  389767.905095 \n",
      "last loss =  389767.905095\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  389485.749332 \n",
      "last loss =  389485.749332\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  389221.281179 \n",
      "last loss =  389221.281179\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  388932.255892 \n",
      "last loss =  388932.255892\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  388652.593978 \n",
      "last loss =  388652.593978\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  388389.666447 \n",
      "last loss =  388389.666447\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  388109.652484 \n",
      "last loss =  388109.652484\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  387815.17826 \n",
      "last loss =  387815.17826\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  387567.44573 \n",
      "last loss =  387567.44573\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  387278.476518 \n",
      "last loss =  387278.476518\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  387023.984579 \n",
      "last loss =  387023.984579\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  386751.274844 \n",
      "last loss =  386751.274844\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  386495.313988 \n",
      "last loss =  386495.313988\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  386258.565796 \n",
      "last loss =  386258.565796\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  386004.151948 \n",
      "last loss =  386004.151948\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  385775.035512 \n",
      "last loss =  385775.035512\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  385542.764887 \n",
      "last loss =  385542.764887\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  385281.536227 \n",
      "last loss =  385281.536227\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  385030.681566 \n",
      "last loss =  385030.681566\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  384810.554793 \n",
      "last loss =  384810.554793\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  384591.989953 \n",
      "last loss =  384591.989953\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  384351.185993 \n",
      "last loss =  384351.185993\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  384111.291157 \n",
      "last loss =  384111.291157\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  383896.434216 \n",
      "last loss =  383896.434216\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  383673.812224 \n",
      "last loss =  383673.812224\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  383459.380331 \n",
      "last loss =  383459.380331\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  383196.759584 \n",
      "last loss =  383196.759584\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  382985.161418 \n",
      "last loss =  382985.161418\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  382759.291805 \n",
      "last loss =  382759.291805\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  382571.841677 \n",
      "last loss =  382571.841677\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  382385.120314 \n",
      "last loss =  382385.120314\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  382198.134771 \n",
      "last loss =  382198.134771\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  382005.114715 \n",
      "last loss =  382005.114715\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  381810.5066 \n",
      "last loss =  381810.5066\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  381614.619849 \n",
      "last loss =  381614.619849\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  381399.977627 \n",
      "last loss =  381399.977627\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  381222.263813 \n",
      "last loss =  381222.263813\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  381032.47896 \n",
      "last loss =  381032.47896\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  380847.80673 \n",
      "last loss =  380847.80673\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  380669.270019 \n",
      "last loss =  380669.270019\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  380469.216525 \n",
      "last loss =  380469.216525\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  380277.591437 \n",
      "last loss =  380277.591437\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  380098.876804 \n",
      "last loss =  380098.876804\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  379928.344583 \n",
      "last loss =  379928.344583\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  379748.398866 \n",
      "last loss =  379748.398866\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  379548.0164 \n",
      "last loss =  379548.0164\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  379370.020273 \n",
      "last loss =  379370.020273\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  379183.027629 \n",
      "last loss =  379183.027629\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  379011.852663 \n",
      "last loss =  379011.852663\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  378849.276726 \n",
      "last loss =  378849.276726\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  378684.921902 \n",
      "last loss =  378684.921902\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  378513.383232 \n",
      "last loss =  378513.383232\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  378343.187822 \n",
      "last loss =  378343.187822\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  378184.275079 \n",
      "last loss =  378184.275079\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  378025.21959 \n",
      "last loss =  378025.21959\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  377874.098265 \n",
      "last loss =  377874.098265\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  451817.532646\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  436825.133532 \n",
      "last loss =  436825.133532\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  427902.052866 \n",
      "last loss =  427902.052866\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  422167.306256 \n",
      "last loss =  422167.306256\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  419052.406573 \n",
      "last loss =  419052.406573\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  416418.181036 \n",
      "last loss =  416418.181036\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  414084.524574 \n",
      "last loss =  414084.524574\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  412360.434324 \n",
      "last loss =  412360.434324\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  411099.296425 \n",
      "last loss =  411099.296425\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  409994.417968 \n",
      "last loss =  409994.417968\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  408975.97269 \n",
      "last loss =  408975.97269\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  408098.795955 \n",
      "last loss =  408098.795955\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  407232.1156 \n",
      "last loss =  407232.1156\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  406490.242929 \n",
      "last loss =  406490.242929\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  405717.668758 \n",
      "last loss =  405717.668758\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  405011.252719 \n",
      "last loss =  405011.252719\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  404400.814212 \n",
      "last loss =  404400.814212\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  403837.166365 \n",
      "last loss =  403837.166365\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  403338.422166 \n",
      "last loss =  403338.422166\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  402815.970868 \n",
      "last loss =  402815.970868\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  402266.343801 \n",
      "last loss =  402266.343801\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  401756.899786 \n",
      "last loss =  401756.899786\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  401242.160795 \n",
      "last loss =  401242.160795\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  400846.253575 \n",
      "last loss =  400846.253575\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  400434.361397 \n",
      "last loss =  400434.361397\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  400040.529399 \n",
      "last loss =  400040.529399\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  399669.744496 \n",
      "last loss =  399669.744496\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  399229.847143 \n",
      "last loss =  399229.847143\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  398883.769098 \n",
      "last loss =  398883.769098\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  398553.607709 \n",
      "last loss =  398553.607709\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  398222.641498 \n",
      "last loss =  398222.641498\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  397910.153948 \n",
      "last loss =  397910.153948\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  397569.696741 \n",
      "last loss =  397569.696741\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  397279.080718 \n",
      "last loss =  397279.080718\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  396985.916626 \n",
      "last loss =  396985.916626\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  396675.394824 \n",
      "last loss =  396675.394824\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  396390.152312 \n",
      "last loss =  396390.152312\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  396081.31293 \n",
      "last loss =  396081.31293\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  395803.424823 \n",
      "last loss =  395803.424823\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  395530.163033 \n",
      "last loss =  395530.163033\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  395257.196584 \n",
      "last loss =  395257.196584\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  394989.335747 \n",
      "last loss =  394989.335747\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  394728.140509 \n",
      "last loss =  394728.140509\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  394479.167832 \n",
      "last loss =  394479.167832\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  394227.444277 \n",
      "last loss =  394227.444277\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  393961.00123 \n",
      "last loss =  393961.00123\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  393719.113441 \n",
      "last loss =  393719.113441\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  393496.11621 \n",
      "last loss =  393496.11621\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  393256.267849 \n",
      "last loss =  393256.267849\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  393007.028728 \n",
      "last loss =  393007.028728\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  392772.73318 \n",
      "last loss =  392772.73318\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  392544.464733 \n",
      "last loss =  392544.464733\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  392325.452394 \n",
      "last loss =  392325.452394\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  392107.601438 \n",
      "last loss =  392107.601438\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  391884.487226 \n",
      "last loss =  391884.487226\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  391687.790513 \n",
      "last loss =  391687.790513\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  391470.153773 \n",
      "last loss =  391470.153773\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  391273.152836 \n",
      "last loss =  391273.152836\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  391069.538702 \n",
      "last loss =  391069.538702\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  390852.392266 \n",
      "last loss =  390852.392266\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  390653.922717 \n",
      "last loss =  390653.922717\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  390451.723514 \n",
      "last loss =  390451.723514\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  390266.162024 \n",
      "last loss =  390266.162024\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  390075.202598 \n",
      "last loss =  390075.202598\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  389885.195867 \n",
      "last loss =  389885.195867\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  389697.546196 \n",
      "last loss =  389697.546196\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  389510.227046 \n",
      "last loss =  389510.227046\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  389319.334785 \n",
      "last loss =  389319.334785\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  389142.624268 \n",
      "last loss =  389142.624268\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  388966.802603 \n",
      "last loss =  388966.802603\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  388780.113055 \n",
      "last loss =  388780.113055\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  388610.831675 \n",
      "last loss =  388610.831675\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  388435.525346 \n",
      "last loss =  388435.525346\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  388275.637188 \n",
      "last loss =  388275.637188\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  388119.10456 \n",
      "last loss =  388119.10456\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  387962.102694 \n",
      "last loss =  387962.102694\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  387800.696706 \n",
      "last loss =  387800.696706\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  387643.100442 \n",
      "last loss =  387643.100442\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  387483.167027 \n",
      "last loss =  387483.167027\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  387326.522959 \n",
      "last loss =  387326.522959\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  387167.070737 \n",
      "last loss =  387167.070737\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  387004.451563 \n",
      "last loss =  387004.451563\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  386858.860279 \n",
      "last loss =  386858.860279\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  386710.376246 \n",
      "last loss =  386710.376246\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  386567.226388 \n",
      "last loss =  386567.226388\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  386401.034186 \n",
      "last loss =  386401.034186\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  386241.188181 \n",
      "last loss =  386241.188181\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  386099.98066 \n",
      "last loss =  386099.98066\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  385963.187551 \n",
      "last loss =  385963.187551\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  385831.896083 \n",
      "last loss =  385831.896083\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  385658.899846 \n",
      "last loss =  385658.899846\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  385524.319201 \n",
      "last loss =  385524.319201\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  385371.991218 \n",
      "last loss =  385371.991218\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  385230.145622 \n",
      "last loss =  385230.145622\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  385080.978319 \n",
      "last loss =  385080.978319\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  384940.406951 \n",
      "last loss =  384940.406951\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  384804.300711 \n",
      "last loss =  384804.300711\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  384672.28754 \n",
      "last loss =  384672.28754\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  384550.652804 \n",
      "last loss =  384550.652804\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  384417.267575 \n",
      "last loss =  384417.267575\n",
      "learning_rate =  0.5\n",
      "sample_size 500\n"
     ]
    }
   ],
   "source": [
    "read = True\n",
    "fname = \"../dumps/last_grid.pcl\"\n",
    "if not read:\n",
    "    trees_grid = {}\n",
    "    for lr in [0.35,0.4,0.5]:\n",
    "            for r in [0,0.001]:\n",
    "                _formula = greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                                      loss = MSELoss,\n",
    "                                                      learning_rate = lr,\n",
    "                                                      learning_rate_decay=1.,# no decay\n",
    "                                                      nTrees = target_n_trees,\n",
    "                                                      trees_sample_size =500, #chosen from the ensemble at random each iteration\n",
    "                                                      verbose = True,\n",
    "                                                      regularizer=r*len(trainFactory.labels), #added to gradient walker's leaf denominator\n",
    "                                                      use_joblib=True,\n",
    "                                                      n_jobs=global_n_jobs, #threading by default\n",
    "                                                      )\n",
    "                trees_grid[(lr,r)] = _formula\n",
    "                \n",
    "    cDump(trees_grid, fname)\n",
    "else:\n",
    "    trees_grid = cLoad(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.35, 0.001) 0.565079919559\n",
      "(0.5, 0.001) 0.566144651233\n",
      "(0.4, 0.001) 0.563709997282\n",
      "(0.4, 0) 0.565228472769\n",
      "(0.35, 0) 0.564526227171\n",
      "(0.5, 0) 0.567501400434\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for key in trees_grid:\n",
    "    _formula = trees_grid[key]\n",
    "    y_pred_sanity = _formula.predict(testFactory)\n",
    "    print key, metrics.mean_squared_error(testFactory.labels,y_pred_sanity)\n",
    "    res.append((key, metrics.mean_squared_error(testFactory.labels,y_pred_sanity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#old hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usability distribution\n",
    "read = True\n",
    "fname = '../dumps/thresholds.pcl'\n",
    "if not read:\n",
    "    thresholds = mnet.get_thresholds(trees,formula.feature_ids,0.001)\n",
    "    #todo: make a LOGICALLY CONSISTENT parallel threshold extractor AND criteria selector. Right now joblib slows things down\n",
    "    cDump(thresholds,fname)\n",
    "else:    \n",
    "    thresholds = cLoad(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(thresholds)),thresholds[:,2])\n",
    "print sum(thresholds[:,2] >150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hierarchy as old_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usability distribution\n",
    "read = False\n",
    "fname = '../dumps/criteria.pcl'\n",
    "if not read:\n",
    "    thresholds_active = thresholds[thresholds[:,2]>100] #at least 100 times used in the original ensemble\n",
    "    print len(thresholds_active)\n",
    "    criteria = hierarchy.select_criteria(trainFactory,thresholds_active,4,True)\n",
    "    cDump(criteria,fname)\n",
    "else:\n",
    "    criteria = cLoad(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split = hierarchy.split_upper(trainFactory,criteria,equalizeWeights=False,split_weights=1.,split_inclusion=.7) \n",
    "#при каждом разделении в подвыборку  попадает split_inclusion примеров из другой половины выборки с весом split_weights\n",
    "#print [split[i].events.shape[0] for i in split]\n",
    "#print [sum(split[i].weights) for i in split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "read = False\n",
    "fname = \"../dumps/last_old_hierarchy.pcl\"\n",
    "if not read:\n",
    "    \n",
    "    trees_splitted = hierarchy.train_splitted_boosts(trees, trainFactory,criteria,\n",
    "                                                     breadth = 1,\n",
    "                                                     loss = MSELoss,\n",
    "                                                     learning_rate = 0.25, \n",
    "                                                     nTrees_leaf= target_n_trees,\n",
    "                                                     trees_sample_size=500,\n",
    "                                                     regularizer =0.0001,\n",
    "                                                     verbose=True,\n",
    "                                                     use_joblib = True,n_jobs = global_n_jobs,\n",
    "                                                     joblib_backend = \"multiprocessing\",\n",
    "                                                     weights_outside_leaf = 0.75, inclusion_outside_leaf = 0.75) \n",
    "    cDump(trees_splitted,fname)\n",
    "else:\n",
    "    trees_splitted = cLoad(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_splitted= trees_splitted.predict(testFactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_test = testFactory.weights\n",
    "Yts = testFactory.labels\n",
    "print 'spltd\\t',metrics.mean_squared_error(Yts,y_pred_splitted)\n",
    "print 'greedy\\t',metrics.mean_squared_error(Yts,y_pred_greedy)\n",
    "print 'stupid\\t',metrics.mean_squared_error(Yts,y_pred_stupid)\n",
    "print 'full\\t',metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MSE learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_curve(formula,factory, metric,n_points = None):\n",
    "    \n",
    "    lcurve = []\n",
    "\n",
    "    Ypred = np.zeros(len(factory.labels))\n",
    "                  \n",
    "    for i,tree_pred in enumerate(formula.staged_predict(factory)):\n",
    "\n",
    "        Ypred += tree_pred\n",
    "        lcurve.append(metric(factory.labels, Ypred,sample_weight = factory.weights))\n",
    "        if n_points is not None and i >= n_points:\n",
    "            break\n",
    "    while n_points is not None and i < n_points:\n",
    "        i+=1\n",
    "        lcurve.append(lcurve[-1])\n",
    "        \n",
    "    return lcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metric_name = 'MSE'\n",
    "n_trees = target_n_trees\n",
    "metric = metrics.mean_squared_error\n",
    "\n",
    "stupid_lcurve = learning_curve(trees_stupid,testFactory,metric,n_trees)\n",
    "greedy_lcurve = learning_curve(trees_greedy,testFactory,metric,n_trees)\n",
    "splitted_lcurve = learning_curve(trees_splitted,testFactory,metric,n_trees)\n",
    "\n",
    "full_line = metric(testFactory.labels,y_pred_full)\n",
    "\n",
    "p = range(n_trees+1)\n",
    "\n",
    "plt.figure(figsize = [14,14])\n",
    "plt.plot(p,[full_line for i in p],label = \"full\")\n",
    "plt.plot(p,stupid_lcurve,label = \"stupid\")\n",
    "plt.plot(p,greedy_lcurve,label = \"greedy\")\n",
    "plt.plot(p,splitted_lcurve,label = \"splitted\")\n",
    "plt.title('learning curves('+metric_name+')')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
