{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#general\n",
    "import copy\n",
    "import random\n",
    "\n",
    "#math & plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "#sklearn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "#EF applier\n",
    "import _matrixnetapplier as mnet\n",
    "\n",
    "#persistance\n",
    "import h5py\n",
    "import cPickle\n",
    "from StringIO import StringIO\n",
    "\n",
    "#debug purposes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997 == 9997\n"
     ]
    }
   ],
   "source": [
    "with open('../formula/MSLR10k_ef.mx', 'r') as f:\n",
    "    formula = mnet.MatrixnetClassifier(StringIO(cPickle.load(f))) #btw he's a regressor, not classifier\n",
    "\n",
    "depth, nTrees, itr = formula.iterate_trees().next()\n",
    "trees = [tree for tree in itr]\n",
    "print len(trees), '==',nTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_as_h5(path_to_txt,output_name=\"mslr\"):\n",
    "    \n",
    "    print \"opening \"+path_to_txt\n",
    "    f = open(path_to_txt)\n",
    "    labels = []\n",
    "    features = []\n",
    "    print \"extracting...\"\n",
    "    for line in f:\n",
    "        line = line[:line.find('#') - 1]#удалить комменты из конца линии\n",
    "        ls = line.split()\n",
    "        labels.append(int(ls[0]))\n",
    "        features.append([float(x[x.find(':') + 1:]) for x in ls[1:]])\n",
    "    f.close()\n",
    "    print \"converting & sorting...\"\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    features = np.asarray(features)\n",
    "    query = features[:, 0].astype(int)\n",
    "    features = features[:, 1:]\n",
    "    sorter = np.argsort(query)\n",
    "    print \"saving...\"\n",
    "    h5f = h5py.File(output_name, 'w')\n",
    "    h5f.create_dataset('qids', data=query[sorter])\n",
    "    h5f.create_dataset('labels', data=labels[sorter])\n",
    "    h5f.create_dataset('features', data=features[sorter])\n",
    "    h5f.close()\n",
    "    print \"done\"\n",
    "    return features,query,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening ../data/MSLR10/Fold1/vali.txt\n",
      "extracting...\n",
      "converting & sorting...\n",
      "saving...\n",
      "done\n",
      "Wall time: 48.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 2.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 3.,  0.,  3., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([   10,    10,    10, ..., 29995, 29995, 29995]),\n",
       " array([0, 0, 1, ..., 1, 2, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "##warning! this can take a long time. no need to rerun that code if u have CSV files created once.\n",
    "#save_as_h5(\"../data/MSLR10/Fold1/train.txt\",\"../data/MSLR10/mslr_train\")\n",
    "#save_as_h5(\"../data/MSLR10/Fold1/test.txt\",\"../data/MSLR10/mslr_test\")\n",
    "#save_as_h5(\"../data/MSLR10/Fold1/vali.txt\",\"../data/MSLR10/mslr_vali\")\n",
    "#print \"converted that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load training set\n",
    "def load_h5(name):\n",
    "    print \"reading from\",name\n",
    "    h5f = h5py.File(name,'r')\n",
    "    labels = h5f['labels'][:]\n",
    "    qids = h5f['qids'][:]\n",
    "    features = h5f['features'][:]\n",
    "    h5f.close()\n",
    "    print \"done\"\n",
    "    return features, qids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from ../data/MSLR10/mslr_vali\n",
      "done\n",
      "reading from ../data/MSLR10/mslr_test\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "Xtr,Qtr,Ytr = load_h5(\"../data/MSLR10/mslr_train\")\n",
    "Xts,Qts,Yts = load_h5(\"../data/MSLR10/mslr_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from factory import RegressionFactory\n",
    "#s_ind = np.arange(1000)\n",
    "#trainFactory = RegressionFactory(Xtr[s_ind],Ytr[s_ind])\n",
    "#DataFactory is just a data wrapper that can handle splits, predictions, etc. \n",
    "#Used to avoid recomputing metadata at each predictions and passing large argument strings\n",
    "trainFactory = RegressionFactory(Xtr,Ytr)\n",
    "testFactory = RegressionFactory(Xts,Yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (723412L, 136L) qids: 6000\n",
      "test:  (241521L, 136L) qids: 2000\n",
      "qid intersection: 0 (must be 0)\n"
     ]
    }
   ],
   "source": [
    "print \"train: \",Xtr.shape,\"qids:\",len(set(Qtr))\n",
    "print \"test: \",Xts.shape,\"qids:\",len(set(Qts))\n",
    "print \"qid intersection:\",len(set.intersection(set(Qtr),set(Qts))),\"(must be 0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy pruning for the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import greedy\n",
    "from loss_functions import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  650951.041929\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  558858.329132 \n",
      "last loss =  558858.329132\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  506327.910468 \n",
      "last loss =  506327.910468\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  475100.0006 \n",
      "last loss =  475100.0006\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  456614.133348 \n",
      "last loss =  456614.133348\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  444930.002983 \n",
      "last loss =  444930.002983\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  438043.675827 \n",
      "last loss =  438043.675827\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  433368.076071 \n",
      "last loss =  433368.076071\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  430523.388357 \n",
      "last loss =  430523.388357\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  428366.077034 \n",
      "last loss =  428366.077034\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  426694.306363 \n",
      "last loss =  426694.306363\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  425380.395735 \n",
      "last loss =  425380.395735\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  424476.072634 \n",
      "last loss =  424476.072634\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  423496.711868 \n",
      "last loss =  423496.711868\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  422838.338198 \n",
      "last loss =  422838.338198\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  422174.366831 \n",
      "last loss =  422174.366831\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  421579.8668 \n",
      "last loss =  421579.8668\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  421029.666608 \n",
      "last loss =  421029.666608\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  420530.902567 \n",
      "last loss =  420530.902567\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  420000.882858 \n",
      "last loss =  420000.882858\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  419557.451784 \n",
      "last loss =  419557.451784\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  419130.990173 \n",
      "last loss =  419130.990173\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  418647.196106 \n",
      "last loss =  418647.196106\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  418238.546269 \n",
      "last loss =  418238.546269\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  417878.9067 \n",
      "last loss =  417878.9067\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  417563.920217 \n",
      "last loss =  417563.920217\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  417194.695588 \n",
      "last loss =  417194.695588\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  416869.432446 \n",
      "last loss =  416869.432446\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  416546.870169 \n",
      "last loss =  416546.870169\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  416237.917014 \n",
      "last loss =  416237.917014\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  415937.68232 \n",
      "last loss =  415937.68232\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  415662.187669 \n",
      "last loss =  415662.187669\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  415383.466592 \n",
      "last loss =  415383.466592\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  415143.509075 \n",
      "last loss =  415143.509075\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  414864.621237 \n",
      "last loss =  414864.621237\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  414592.014622 \n",
      "last loss =  414592.014622\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  414356.562573 \n",
      "last loss =  414356.562573\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  414112.859473 \n",
      "last loss =  414112.859473\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  413856.167322 \n",
      "last loss =  413856.167322\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  413608.031458 \n",
      "last loss =  413608.031458\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  413360.908928 \n",
      "last loss =  413360.908928\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  413143.412063 \n",
      "last loss =  413143.412063\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  412915.689489 \n",
      "last loss =  412915.689489\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  412666.606352 \n",
      "last loss =  412666.606352\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  412434.864587 \n",
      "last loss =  412434.864587\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  412224.918866 \n",
      "last loss =  412224.918866\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  412030.510982 \n",
      "last loss =  412030.510982\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  411834.805418 \n",
      "last loss =  411834.805418\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  411618.445525 \n",
      "last loss =  411618.445525\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  411401.400291 \n",
      "last loss =  411401.400291\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  411217.132458 \n",
      "last loss =  411217.132458\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  411016.039944 \n",
      "last loss =  411016.039944\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  410812.096245 \n",
      "last loss =  410812.096245\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  410603.438344 \n",
      "last loss =  410603.438344\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  410418.861468 \n",
      "last loss =  410418.861468\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  410231.917546 \n",
      "last loss =  410231.917546\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  410034.091882 \n",
      "last loss =  410034.091882\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  409854.879053 \n",
      "last loss =  409854.879053\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  409680.203992 \n",
      "last loss =  409680.203992\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  409498.877918 \n",
      "last loss =  409498.877918\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  409326.974855 \n",
      "last loss =  409326.974855\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  409142.749227 \n",
      "last loss =  409142.749227\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  408992.90545 \n",
      "last loss =  408992.90545\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  408834.426405 \n",
      "last loss =  408834.426405\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  408657.596476 \n",
      "last loss =  408657.596476\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  408487.506193 \n",
      "last loss =  408487.506193\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  408321.32583 \n",
      "last loss =  408321.32583\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  408146.386275 \n",
      "last loss =  408146.386275\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  407990.828557 \n",
      "last loss =  407990.828557\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  407820.963302 \n",
      "last loss =  407820.963302\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  407642.758809 \n",
      "last loss =  407642.758809\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  407465.735274 \n",
      "last loss =  407465.735274\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  407289.386709 \n",
      "last loss =  407289.386709\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  407138.437571 \n",
      "last loss =  407138.437571\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  406987.372399 \n",
      "last loss =  406987.372399\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  406849.490222 \n",
      "last loss =  406849.490222\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  406683.422559 \n",
      "last loss =  406683.422559\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  406526.456114 \n",
      "last loss =  406526.456114\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  406374.88042 \n",
      "last loss =  406374.88042\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  406234.825073 \n",
      "last loss =  406234.825073\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  406091.68402 \n",
      "last loss =  406091.68402\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  405920.166536 \n",
      "last loss =  405920.166536\n",
      "learning_rate =  0.25\n",
      "sample_size 200\n",
      "\n",
      "iteration #"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_greedy = greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                          loss = MSELoss,\n",
    "                                          learning_rate = .005,\n",
    "                                          learning_rate_decay=1.,# no decay\n",
    "                                          nTrees =100,\n",
    "                                          trees_sample_size =200, #chosen from the ensemble at random each iteration\n",
    "                                          verbose = True,\n",
    "                                          regularizer=0.0004, #added to gradient walker's leaf denominator\n",
    "                                          use_joblib=False,\n",
    "                                          n_jobs=-1,\n",
    "                                          joblib_method=\"threads\" #every GIL-ly thing is copied anyways\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_stupid = greedy.predict(testFactory,trees[:100])\n",
    "y_pred_full = greedy.predict(testFactory,trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_greedy = testFactory.predict(res_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824874656293 0.655164092015 0.841985849903\n",
      "well...\n"
     ]
    }
   ],
   "source": [
    "print metrics.mean_squared_error(Yts,y_pred_greedy),\n",
    "print metrics.mean_squared_error(Yts,y_pred_stupid),\n",
    "print metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:83: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:95: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "C:\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:117: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#usability distribution\n",
    "thresholds = mnet.get_thresholds(trees,30,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJztnX+YXVV57z/vJJNJZobJZBJNCAmQSihEqAqtpI0t6SMh\n",
       "aCs/5D5ib/Wh4EUttVEztJJUrzxPvVRaiJinF3mkGNAWKtcfFHu9JFE73tZW0QiaEiJJLUrCTZCE\n",
       "MCQhySTz3j/W2tl7nzkzc86cc+acOfP9PM9+ztlrr732Wmvv/b5rve9aa5u7I4QQQiS01DsDQggh\n",
       "GgspBiGEEDmkGIQQQuSQYhBCCJFDikEIIUQOKQYhhBA5SlIMZvY5M9trZlszYX9lZk+Z2Y/M7Ctm\n",
       "NjNzbI2Z7TCz7WZ2aSb8QjPbGo99urpFEUIIUQ1K7TFsAC4rCNsEvNbdXwc8DawBMLMlwDXAknjO\n",
       "XWZm8ZzPAO9x98XAYjMrTFMIIUSdKUkxuPs/Ay8WhG1298G4+z1gQfx/BfCguw+4+zPATuAiMzsV\n",
       "OMXdH4vxPg9cWWH+hRBCVJlq+RiuB74e/88HdmWO7QJOKxK+O4YLIYRoICpWDGb2Z8Axd3+gCvkR\n",
       "QghRZ6ZWcrKZ/QHwVuDNmeDdwMLM/gJCT2E3qbkpCd89TLpawEkIIcaAu9vosUZmzIohOo7/BLjY\n",
       "3Y9kDj0CPGBm6wimosXAY+7uZtZvZhcBjwHvBtYPl341CteomNkt7n5LvfNRC5q5bKDyTXQmQfmq\n",
       "0qguSTGY2YPAxcAcM3sW+DhhFNI0YHMcdPRv7n6ju28zs4eAbcBx4EZPl3C9EbgPmAF83d0frUYh\n",
       "hBBCVI+SFIO7/16R4M+NEP9W4NYi4VuA80vOnRBCiHFHM5/rQ1+9M1BD+uqdgRrTV+8M1Ji+emeg\n",
       "xvTVOwMTAWvED/WYmTezj0EIIWpBtWSnegxCCCFySDEIIYTIIcUghBAihxSDEEKIHFIMQgghckgx\n",
       "CCGEyCHFIIQQIocUgxBCiBxSDEIIIXJIMYiyMLOVZrM3hc1W1js/QojqoyUxJhlBmPf0hr39fdCz\n",
       "PP6/w9035uMcnQ2tQMs+2H9HiNf1VVg/I/xf9Qr0X5WcJ4SoL9WSnVIMk4gg8BPBvhW4h/BJjK3A\n",
       "vYNweA+0zIZpbXADcD9wezx71SvQ8hTceQFcG8PuB1Zvdt936bgXRggxBK2VJMZAT29QCtcCPyAo\n",
       "hXnA3wJvboGp82F6Wwj/T4JSuDZu62dAyxn59LYCXCCzkhDNhRTDpGQj8O/x/2eB24AngNNJH4l9\n",
       "BfHvBo61wqqjcBNwLqHHsW42rFsBXV+VchCiOZApaRKRmpKWzIBlhJ7CIuD9wJ8DR4FrgHuBY4Tv\n",
       "OBWalN43AG1TYYmF82RWEqJRkClJlE1wEvd/An4yED6kdz8wBfgQ8HrgZUL464BfISiFh8iblN7Q\n",
       "CusN5tejCEKIcaCkT3uK5iD2GD4K17cGc9DtBOH/vgH41hE4PA1WtQWTUmc86+Awqb2XtLcAcYTS\n",
       "HbXLvRBivJApaRJhNntT8AdcS/Ab3AL8ez+0tMH6thDrfQPAVJhiMB14NfACmdFJAB56DcloJp6A\n",
       "A2s1bFWI+lIt2akew6RlJbAHWD0A67rS1v/dreF3GcGM1A38DvBIPH4DcPfjsDp6pw/cIYUgRHMh\n",
       "xTCp2H8HrHoTkJmgZi8Cs4fGPR9YAbwT2ElQHN8BvjUIh77sfvDW8cmzEGK8kSlpkjF05nP7f4f2\n",
       "tsyoo+MwZSq0E8K2An/t0GZhfgNoxrMQjcm4jkoys8+Z2V4z25oJ6zGzzWb2tJltMrPuzLE1ZrbD\n",
       "zLab2aWZ8AvNbGs89ulKMy/Kx903uu+7NAwr7VkOd7WFYauPEOYqzHgF/hD4ZcIQ1o3AjINBKWQn\n",
       "u/X0at0kIZqTUoerbgAuKwi7Gdjs7mcD34z7mNkSwmD4JfGcu8ws0WCfAd7j7ouBxWZWmKYYVwaj\n",
       "CWkl8GXCvAQjmJH+hWBCugloOTb03KOzw5yIdSs0wU2I5qIkH4O7/7OZnVkQfDlwcfx/P9BHUA5X\n",
       "AA+6+wDwjJntBC4ys58Bp7j7Y/GczwNXAo9WUgBRCQMEwZ9wE3B4L6yaSs4P0b8OVn00H9YK3Dkj\n",
       "M2R1BqzuJXQxhBATmEqcz3PdfW/8vxeYG//PB76bibcLOI0ghXZlwnfHcDHOpH6GtjOCYL+HMEKp\n",
       "Azg+FwaeCpPeWvZB/x3uvtHMtkTBT5ivkPgphBDNRlVGJbm7m1lVvdhmdktmt8/d+6qZ/mQlXRZj\n",
       "XVxh9TMER/O1hI7fnacAF4RewYGTDub4uzGTDkNHOGmCmxDjiZktB5ZXO91KFMNeM5vn7nvM7FTg\n",
       "+Ri+G1iYibeA0FPYHf9nw3cPl7i731JB3sSwdN+aNwFtJJiQHiFd+gKAGfBHt5rNTkYw5eYrxF7E\n",
       "VaEXkXy3oafXzBjLaCUz2wCzfj/svfhP0GNpugNkvwsRr13kuxJHZ4N3wfRZMPgzTboTzU5sMPcl\n",
       "+2b28WqkW4lieIQgRW6Lvw9nwh8ws3UEU9Fi4LHYq+g3s4uAx4B3k45/FONAEKbdr8+HdhaPzFZg\n",
       "yuthXRygcONvmc16Miuco4C+ELr+HO6M8Va9ycxKHsoa8jT1c9A1Hz4Vr3vPpXAdoQeT9GSS4bQ3\n",
       "/pZZx7PQ9Ushb1uBz6wIM7SfJR1my2xY9Q9mdoWUgxBl4u6jbsCDwHOEJTefJby1PcA3gKeBTUB3\n",
       "Jv5awpCW7cDKTPiFhDd5J7B+hOt5KfnSVt4GPZvgaocuh/vi1u7QdQx6HeZkwrtPhF93eLTgWPsR\n",
       "6N4CHVvy8Twe79lU4nO1EroOw2xP07g4/n97wW+Sjx6HpZmwZTFvSwvCy8uLNm3NsFVLdpY6Kun3\n",
       "hjl0yTDxbwWGzIx19y2EsZCibvyEsKxFssTFW4CNr8CGfjjyInyoP/QKmA1cEOJ8ltTMtJE4Ie6C\n",
       "MO9huEX2SqGnN/g6sn7sXcPGhr8A1mXyDsFpfnsMe66CvAghErQkxqRifx8Mrgi6+XaCkH8X8Ndd\n",
       "4fiqdjjwAT9pw1/1VWBGXuBmlcQ9wM/JD3ld5eU7oU8hLs5H8CXcFNPP/gL8R/zNruw6IxP2zsK8\n",
       "HJVDXIjy0ZIYk4iwuup1K8JM59sILf7hP7aTOniPzoYprw0rsGbPWU6wKs4jKIzngB/vdD+0uLT8\n",
       "JCOkVsyAfwReBRwBDhO+B7ELOAAwCAyAt8KMlnSpjg3A0cMwpRXWt4awuwfBjkDrCfAdckCLyYQ+\n",
       "1CPGSPKBnkcY2WyTXT7j4IXQfwWs3gxbfxha4vcTvvgG+ZnT0/+z1Jz4yQ8HbR6EDxAGqh0fhMP3\n",
       "hevsHwxLf7+vJSwNPqUluLnuJizod2QADr0d+t8W8rZhMxz6GLQYfOoUuPMCzcgWonzUY5hEpC30\n",
       "9dH+cuPRYE1MvsVQ2uJ4xXsSpZ+fT6vYNyKe3gcnXoRzzwrKJvEfvJ/QO/lz4GfAwZfhwCfDUFUI\n",
       "q8fO/Gv49Fn65KiYjOh7DKJsPDf3AOBwtL+nM5pLEeqemewWlER55xdnI2EE818BW2fDvUWWAk/Y\n",
       "SfCN/O9TYPB/BIc0wA2/DaZnWogKUY9B1JW0F3P6jNTZfDVhMb//Q2i7dAH7CXMUziJ8ROhvgUWk\n",
       "/o6NwH8l+DwSHwoEp3b/ZfIziMmAfAyiKYh+hqtg90D+yG7CUNoWgu8hWQr8pwT/wm2EZbkSPktQ\n",
       "HFkfyt3AYL+UghDlIcUg6krqrxh4JfQY/gth3uR2wpyLOwkK4HzgY4SJ9Nvj2e8FPkI47zvARYQe\n",
       "wh7C4r/bgIO3IYQoC9ljRd0YuqDfeoL5qJ3QO3gyxryAMGppOsFs9COCMrgdeBNhIcCpwLcJnyO9\n",
       "CTgBwWO9ZbzKI0SzIB+DqBv5EUkApwJnko5EWgTcSxD680gX+7ucdO7EYwRTU3bE0pOEngboM6Ri\n",
       "MiEfg2hCkg7sVkKv4HzgdYSewdkFcVcSTEkDBWFzCUoh/xnSGmZaiKZDpiRRR/bfkf+mwy8G4Bet\n",
       "wTdwA6GHcFaMmyyD8a4YvpWwJAeE+MlSGFovSYhKkSlJ1JWC7yoUTFDbSPiS3HME/8NW4N5BOPZT\n",
       "aJsLv3zK0ElvLx6GKVMqmXQnxESlWrJTikE0FEP9DvcDf/TD8L/tjOQDPEGZzF+Rzn1I4q7eHBRM\n",
       "qmykFMRkQYpBNCVDl+1Y9UpYT6nrowVhX4T2P8h8mAdYNQD9b5MiEJMVKQbRtAw1L/X0Du1FrN4H\n",
       "62bnV3bd+sOw4J8QkxONShJNS7qq675LR2/9Z1d2bds3DtkTounRqCRRdwp7COmHgk6G9eVHL616\n",
       "BfrXwaqPFoTpozxCVAGZkkRdKcOn8Ins8tpFlIeczGLSIx+DaAqKj0JK/Af6poIQ5SAfgxBCiJog\n",
       "H4OoM4Wzn+U/EKLeyJQk6k4Jzmf5D4QogYbxMZjZGsICNoOENQuuAzqALwJnAM8A73D3A5n41xPW\n",
       "RV7l7puKpCnFIIQQZdIQPgYzO5Ow2tkF7n4+MAV4J3AzsNndzwa+GfcxsyXANcAS4DLgLjOTn0MI\n",
       "IRqISoVyP2Hd43Yzm0pYn+A5woL598c49wNXxv9XAA+6+4C7P0P4qvsbK8yDEEKIKlKRYnD3/cAd\n",
       "wM8JCuGAu28G5rr73hhtL2GRfAjfaNyVSWIXcFoleRBCCFFdKhqVZGavIayLfCbwEvC/zOxd2Tju\n",
       "7mY2kiOj6DEzuyWz2+fufZXkVQghmg0zWw4sr3a6lQ5X/VXgX919H4CZfQX4dWCPmc1z9z1mdirw\n",
       "fIy/G1iYOX9BDBuCu99SYd6EEKKpiQ3mvmTfzD5ejXQr9TFsB5aa2QwzM+ASwue0vkY6bfVa4OH4\n",
       "/xHgnWY2zcwWAYsJH+0VQgjRIFTUY3D3H5nZ54EfEIar/pCwBvIpwENm9h7icNUYf5uZPURQHseB\n",
       "G70RJ1IIIcQkRhPchBCiSWiIeQxCCCGaDykGIYQQOaQYhBBC5JBimISY2Uqz2ZvCZivrnR8hRGMh\n",
       "5/MkY5gvpl2l1UuFmPhUS3bqewyTjp5eWDcjnWbCDFjdC0gxCCEAmZKEEEIUoB7DpKPoF9P0dTQh\n",
       "xEnkY5iE6OtoQjQnDfMFt1ogxSCEEOWjmc9CCCFqghSDEEKIHFIMQgghckgxCCGEyCHFIMaEltUQ\n",
       "onnRqCRRNvVaVkPDbIUYGS2JIerI+C+rkSqjdYkyepOZaY0nIWqAFIOYIGiNJyHGCykGMQa0rIYQ\n",
       "zYx8DGJMjLe9X8uFCzE6WhJDTDrkfBZiZKQYxLgjwSxEYyPFIMYVmXKEaHwaZhE9M+s2sy+Z2VNm\n",
       "ts3MLjKzHjPbbGZPm9kmM+vOxF9jZjvMbLuZXVrp9cV40dMblMK1hG39jLT3IIRoJqox8/nTwNfd\n",
       "/VzgV4DtwM3AZnc/G/hm3MfMlgDXAEuAy4C7zEyzr4UQooGoyJRkZjOBx939lwrCtwMXu/teM5sH\n",
       "9Ln7OWa2Bhh099tivEeBW9z9uwXny5TUYMiUJETj0ygznxcBvzCzDcDrgC3Ah4C57r43xtkLzI3/\n",
       "5wNZJbALOK3CPIhxwN03mtlVcVIZ0C/nsxBNSqWKYSpwAfABd/++md1JNBsluLub2UjdkqLHzOyW\n",
       "zG6fu/dVmFdRIVERSBkI0SCY2XJgebXTrVQx7AJ2ufv34/6XgDXAHjOb5+57zOxU4Pl4fDewMHP+\n",
       "ghg2BHe/pcK8iSZFw2aFCMQGc1+yb2Yfr0a6FTl+3X0P8KyZnR2DLgGeBL5GuqjNtcDD8f8jwDvN\n",
       "bJqZLQIWA49VkgcxucgsprcibF1f1bLfQlSXaqyV9MfA35nZNOA/gOuAKcBDZvYe4BngHQDuvs3M\n",
       "HgK2AceBG70RJ1KIBkaL6QlRaypWDO7+I+DXihy6ZJj4twK3VnpdUT1kmhFCZNHM50nORBuGWkp+\n",
       "pejEZEVLYoiqYDZ7U7DVJ6aZ+4HVm933Neys9JEE/0RTdEJUk0aZxyCaho3AZ4HngKOzi8VolJb4\n",
       "yMNm5YMQolK0HMWkZ/8dcONReBdwOfB+YMprC0f6aDSQEJMH9RgmOWFG86wn4fYLMq3stqGt7InS\n",
       "EtfX5YSoFCkGAbTsq3cOqoWW7hCicuR8FmWM9JFTV4hGRqOSRFUpxbHcKM5nIURxpBiEEELkaJgv\n",
       "uAkhhGgupBiEEELkkGIQY8LMVprN3hQ2zWcQopmQj0GUjUYoCdGYaEkMUUcmymQ3IcRYkClJNAUy\n",
       "bQlRPWRKEmUzVlNSreZByLQlREDzGMS4UyDY+6Bnefw/qpCv5XcUJuLS4ULUAvkYxLiSWV01Eexv\n",
       "gv1ltMpH9ksUS9/M1OoXog5IMYgSqbXDuZL0taKqENVEikGME7UT3lpRVYjqIh+DKIlqOHj1SU4h\n",
       "aoucz2LcqfXqqlq9VYjKkGIQQgiRo6FWVzWzKWb2uJl9Le73mNlmM3vazDaZWXcm7hoz22Fm281M\n",
       "wwmFEKLBqNbM5w8C24Ck+3EzsNndzwa+GfcxsyXANcAS4DLgLjPT7GshhGggKhbKZrYAeCvwN0DS\n",
       "hbmcMMuI+Htl/H8F8KC7D7j7M8BO4I2V5kFUn1KXmNBSFEI0H9UYrvop4E+ArkzYXHffG//vBebG\n",
       "//OB72bi7QJOq0IeRBUpdbKZJqUJ0ZxUpBjM7HeB5939cTNbXiyOu7uZjeThLnrMzG7J7Pa5e99Y\n",
       "8ynKpdTJZlplVYh6EuXu8mqnW2mP4TeAy83srcB0oMvMvgDsNbN57r7HzE4Fno/xdwMLM+cviGFD\n",
       "cPdbKsybEEI0NbHB3Jfsm9nHq5Fu1YarmtnFwE3u/jYz+0tgn7vfZmY3A93ufnN0Pj9A8CucBnwD\n",
       "OMsLMqHhqvWl1MlmmpQmRGPRcPMYomLodffLzawHeAg4HXgGeIe7H4jx1gLXA8eBDxYTIlIM9afU\n",
       "yWaalCZE49BwiqGaSDFMLOqpHKSYhEiRYhANQT3NSTJlCZFH32MQDUI9RyZpVJQQtUCzjoUQQuRQ\n",
       "j0FUSD0/kqMP9AhRC+RjEBUj57MQjYGcz6JuSBgL0ZhIMYi60MgjgaSwxGRHo5JEnWjMkUBa0E+I\n",
       "6iHFIJqExlRYQkxEpBhEmezvg1Vv5uRQZ40EEqLZkGIQJRPNNR+FG1rgbmD7IPR/ojHMNRq6KkS1\n",
       "kPNZlIzZ7E2wbkVqrrkfWL3ZfV9DfLtbzmcx2ZHzWYgCoiKQMhCiQrQkhiiD/XfAjUfh14FzgQ8O\n",
       "wuDs8N3n2nz7Wd+UFmL8UY9BlMlUYBnBjPTpFuACuPEfQvi6thCnOkNFyxmCKjOSENVDikGUQU9v\n",
       "EP6PALeT+hruboP3U/2hoqUNQdUcBiGqixSDaAI0h0GIaiLFIEoitMo7ZsOqwTBc9abM0R8fhVUA\n",
       "iSmpSkNFNQRViHqg4apiVPLrI20F7h2EYz+Faf3Qsi8IcKiFjb8U30Ejr98kxHiiRfTEuNHo8xdA\n",
       "zmchQPMYhMihOQxCVA8pBlECsvULMZmQKUmUxEQx1UyUfApRCxrCx2BmC4HPA68GHPisu683sx7g\n",
       "i8AZwDPAO9z9QDxnDXA9cAJY5e6biqQrxSDKRk5oMdlpFMUwD5jn7k+YWSewBbgSuA54wd3/0sw+\n",
       "Asxy95vNbAnwAPBrwGnAN4Cz3X2wIF0pBlE2E8FJLkQtqZbsrGitJHff4+5PxP8HgacIAv9ywltJ\n",
       "/L0y/r8CeNDdB9z9GWAn8MZK8iDGTinrEGmtIiEmH1VzPpvZmcAbgO8Bc919bzy0F5gb/88Hvps5\n",
       "bRdBkYhxppRlJKq51MT42P7lJBeiGlRFMUQz0peBD7r7y2ZpT8bd3cxGslcVPWZmt2R2+9y9rwpZ\n",
       "FScpZRmJ6iw1MV5rGbn7RjO7KuYR6JfzWTQ1ZrYcWF7tdCtWDGbWSlAKX3D3h2PwXjOb5+57zOxU\n",
       "4PkYvhtYmDl9QQwbgrvfUmneRKUMzi4tbDTGby0jzWcQk4nYYO5L9s3s49VItyLFYKFrcC+wzd3v\n",
       "zBx6hCAFbou/D2fCHzCzdQQT0mLgsUryIMZKKWaXAfJrIt0Uw1I0PFSI5qPSUUlvAv4v8GNSk9Aa\n",
       "grB/CDidocNV1xKGqx4nmJ6KrX2jUUnjwGhCPYzyuW4F/GcMWQRsODnKp9ThoRpGKsT40BDDVWuF\n",
       "FENjMJpAL2d4qHoWQtQerZUkak41nbmy/QsxcVCPQYwZmYiEaCxkShJ1o8As1Ac9y+P/In6K0uOO\n",
       "4doySQmRQYpB1IXhegnhfyqww2/HrTDl9bC+JXzg5x5gPdnzyhHs6qGIatNsDY2qyU53b7gtZKv+\n",
       "+dCWuycroWcT9LwA9zl43O5z6NgCXYfD//sc2o9A1xFY6mnct/vQ83o2lZeHnk2VptGIW6ZuNwEr\n",
       "yz2urZJ6zz63XYcnev1WS3bK+SxGJT9z+e4iMdrOyE9gu7sN3k+YtiJGYrRZ4eM1a3xyMn4TLyca\n",
       "UgyiBLIv0DzgXZljq16Blp8BRWZEv5f0pVsErMocu/EoTJsdhryW2oVvxrWQRhNOlQuvZjOXiNoj\n",
       "xSDKZCVBSK3eB/wwFcyrvspJgf3jo0EJrG8LSuRDg8AT0P9lWL0cjs6Gqa+FOy+I55bUCnathVQ2\n",
       "6nGMRDM2NKpEvW1itbSTaava/RjVFkuBHRxYG/wRPS8Aa/NxK/cVFF6v3nVUq7otpe5HTr85/TLV\n",
       "rf+J/xxlyuNVSafeBall4bRV9Z6U/AKNLuwSYfWowzKHBQ7d/UMF4snrrQ0O7p4XoHtL2O86DFfH\n",
       "c2c5sGGivuSj5bvUchWLJ8UwuTYpBm21vgejCqPh4owmjMJ57Uegy2GOZxTIkdjbyCiWXof2gnjd\n",
       "J4JS6MqEtTt0HSunV1PvOq7+/RqqjCvtcUyErZnv6xjqwquSTr0LUsvCaRtz/ZdoOioep5RWamj5\n",
       "L42C/+1x6/X0Bc8Oc80Oe3WH82JPIRtWGKeYMmpeATlSnVciOKspdGshwKtxX5tJsVRLdsr5LIpQ\n",
       "6Ud89vfBqjfD1hb4DrB1EKYuyo9AatkHBwgL790ek7iJ4Jhu2zd83jYCe4DB4aOMuUzNiWfWqUo+\n",
       "1RqOlDJrvTLHdToi6uhs6HotrGsba1rFqey+yjlfHCkGUVXii/ZRWNESPtXxHmBnC1x7FnznLNj+\n",
       "ZjP7GNAHJ1bAp8m81MCHyI8WWQQ8SlAaWwmK5CxgCvnhrz8GVg0ArWG/viNMig0Rre2w0dFH2OSF\n",
       "4FbgnhWwLh4tJhCrKXTvJsxtKS+t0etsLB+OyjJ5GwwjUu+uTy27Q9rGXP9jNiWlJo1kpnNiIpqb\n",
       "9QecCP6E84Y1/0Ax5/OsY6nTeq6X4nxOwzq2hGvW3pQ0TN2srbUpq1j588cLTXSjmfsqc1yXe71y\n",
       "n0NO+qqG+qnGlsfyy9hoW7VkZ90LUsvCaavoHozJ+VxcMST2/0fj/gLPC/jShGX+JX40ptvzwsj5\n",
       "K1yqo31HZnRTCaOritdBXuF0b8nXQfeWIsKmyFIi4yuA8qPBFhYR1N1bRq6/rB+plOej8H7lBPio\n",
       "inH0QQzZ8iTPWceWkdMcMqy6qXxPUgzaGnJLX7TeKAiuduj2tNfQ63nH8egCfmjaY1EkHq/ddaKU\n",
       "80cXikkZewpbq2vDqKnRFENvDEt6RLV3fuZb2OfE+9Ob1L9D247i5xTrhY1+H4or5rwSHTm/pSqG\n",
       "4sdLvaelKLmJskkxTMCtuHmktNbrRNo4Obmt8yWYdSjMVZhVoBzKaz0OU4dltjhHHrk08rlZM1dy\n",
       "7OIi6c3qH2o66zoRTUlH0iG4yVDb7P+RhdXQ56d0YZae292f9uSujvcla+I7KbjXDt8jmrmj9Hqs\n",
       "dERU9Sb/NZvZaJg686qkU++C1LJwjbSlrbWlsbVWODa/PNtoedcdv9ZQvjW91KEzCr7uAsFcvKdQ\n",
       "rfyGdDq25HsIhS35qx06B4LvYuaOfD5KUQyv8XS47TIPPpNZg0PNZrOOByXZPhDKnO0xXVxEubTt\n",
       "KNKzyfgoEmWS1HHHiZD/4gojL0CzZr3Zmf9LHGbG/WSOSNLby7b4p+1Ow4bWzXg/v8OY9IpMiMyu\n",
       "DDz8xMqJvkkxTLAtPKiJIljqIzleq1iP424/DS9gb0YRzPF0vkIxgZLatYfJb9lmlpjOkXDNczy0\n",
       "imfuyAvXZQXKudehYxA6D4VWf/uOvLM68U90vgTT4+S8BRkBnZiUshPvsv+zAvk18f8nPO1JLYvh\n",
       "ZwxTT1mhliikwt5XMhlwadxaB0J5ugfT9D6RST9Jp8PTnkOiMHoziiN73ryY11wvxylY9mTkezOW\n",
       "nk7hJMrC3tNwEyKzSjRpkLV7XuHVplFWHzmDVyWdehekloVrpC1vYz4/I0jG7jgbPbz0rnPpL2Ap\n",
       "Dses/brHU8XQ6QW9JA8ttsQROHafQD4P3VtSM0kx5dCxJexnhXV3zE9XRrC2DYS0OrZA+7H0eCKQ\n",
       "53nepFRstFRSnmR01syYn45YN1dnBFki7ItO/HshbcFnGxYXe/65StJZENPtLHjWuuP1u+LxDs/3\n",
       "5pJncWkmLKvAkvQK89exZbTnpHzTz7A+gbXD9wQv9nzP9PWeNkrOKaiP/Hsx3r3r2sgZvCrp1Lsg\n",
       "tSxcI23pSJVHMy9Zd6GgPFLwUmXsvO3DmBdaXihmow7XLE0xpC3sfAsq/2IWtYUX5K/zpdjizgit\n",
       "pJw9UZglreNTPTVdJOkVjuYpzSdQ+EKHfPR4vo4TYd3pMPNY+J8Vhud5aK1n70e7Q+eJ0OJOTIBd\n",
       "ngr0JP9JDyApb+JfSBRHYqrJKoA58ZqJAO51mBHzm7TIr47xZjlMO5TW/7Xx2r2eb+knZX5VLOt5\n",
       "mbTnxjpP6iJ5Bns8VXRzPYxWSnoLidLqyaT1Gs8r1CSvhT6Tk2auQWjfHRTbzEPFZroPfx+LPb8d\n",
       "W4Ii6I11+uqCOnh1Qf56Mvdtjufve7ZR1raj2DtQb7lRL9lZ94LUsnCNtKXCN/tinV3wgJ7j0DWY\n",
       "N0MkL1rhSJ5zHFozwsm9UHgObXG1HQtmkp4XgB/A9OPhpeoezAuwOR7s4omTMWv+SK6TtOaT/BUK\n",
       "zEKhtdAZSdAeAAAP4klEQVTTrvzMgvQSf8P0Q2n5C1uC+bIV1GtBq7L7SNqqXpoRXon5qDsj9GZ5\n",
       "UAhJCz7J7+nxeJunLepEqCTnJj2gRPBkW97XxvvTHo+1eSqM2zztOXTHfJ4a00uud3Xm3DmZ/GV7\n",
       "HZ2Z+lwY6zSp19M9VQ69cT+5ftJbOi9us+I1To/XmROfg2zeujL72bwVtsCTFvqrMvU9y/Omu6z/\n",
       "aeahoeagbEMh25DozJQ9uX6icJP7ekZmP7k32V7rQh/aS1vqqWLLPmvdI/bgG3Gb0IoBuAzYDuwA\n",
       "PlKrwjXaFoTxzPhgTvf0/6z4mwjW5AXI2sITm+i1nrbiEmGcdPenx3Nb49aZ7B+HKYPpw7/MgwBK\n",
       "9hd4amvO9gqyw0wLW++FpofzMul2eOp0Tl6+xHySCL2kPIn5pdfzL2r3CWDDaKaH4q3K7oH0GqfG\n",
       "OlrmqWA7J+azNf6eEvOQtbl3e2h9JkouEURJzyIRronQTYTRzMy1Ts2k0x2vsSD+XxCvM91TpTHH\n",
       "09b5PE+Vf3K/F3i+BT8zc787Ms9Jr8OUmG5yn7tj/CTtpLewrCDPSz0VtjPjNZM6mFZw/XmeV0aX\n",
       "xHKcl0nvNfF+F977JZ63/Z+SeR5elSlXYu5K7lVXvO7czDWyPaDueM70TF0kZqRsbynJY1L2rNkv\n",
       "eY56Xqi3zBiDjKmK7KxHxqcAO4EzCcsXPAGcW4vCNdIWWkRJizB5uRKBcXXmJc4+/L0ZwZG86MlL\n",
       "MtPTFmSrg8V0p2ReqDaHFk9bocmD/2rP90CWeSp4CnsFhQK8sDWfKIZsq/Y+h/meN3skJpcFcUt6\n",
       "RFkFU77dt7hi6HwpL4TmZIRDl6etyHme+gIWeBD4nZ4q2CROovCSbZanyqwjE3ZOPHdWJt0zPG2x\n",
       "JgIoWQQw6T3Oi+GvzhxLrjEnE+c8Dyae5DozPZ2DkLT+u+OxjszxRAkk5U56eNnnLylvksfXeOqb\n",
       "OS/uJ2VOlFhSV62ePp+dniqBpDeywFNFlyiFpFGUPLNJPvBUsHd5aOEn9ZG8P8n1sz2CpHeY9HQS\n",
       "xZH0DBLllrxfSdmzSjxrRpzjo/n8GnGrluxsYfx5I7DT3Z9x9wHg74Er6pCPcaZnNUwHTiEsUeWE\n",
       "heBmA98kHOsEumP8fcB6wtI3p8StM+53xv3jwDMEXdsNvAroAmYC7TH9mUAbYCPk7WMUX5TufOJF\n",
       "CC6H+wlr3qzeBwc+FtbjWURYp+h4vM7Z8dwphPV4/pF0PaTXxWOHgN3ADcAvRsgXuPtG932Xhq3Y\n",
       "2kL77wj5uD/mb9UrcPC2UN6fkNbZVNKlwfYQ6uc4cCTmdT/QT1q/c4FpwGFgG6G+O4BZwGnxd0NM\n",
       "syP+HiSs49QBHIvxjsRjywidZIvXfZlwvwDmAEviOcfjNVuBo/H4rhj/OcJz8zsxnzPiNYl5PkB4\n",
       "jpJ8nhvDPgOcIDwjpwAthLpvI3yq9dvxf3ssawfwUozfFtPYH9P4DnB9/N8Rt7nkn7nWWOYDwADh\n",
       "fnum/v9fjLMZODWe+1rCN8JnxXrxmM9D8b4MxPuxON6vV+K1BoD5wPPA4zHt6TF/P4lpHI7pfTHm\n",
       "ITlvHvBkTHd/jHd33A4Dh77MJKUeiuE04NnM/q4YNgmYGX+XEF6o44QXsIXwohjhgZ5KUBiJUIdU\n",
       "mUwlFfI3AN8ivJBGeImSY4lSmEIqvFYRhOdigkBO9vcQXrRfeBp2fzx+4G9TwbsH2PYK7P99d78V\n",
       "+q+CDZth2sshv1MIAvAj8Xob4rUS3ksQZEsIQvJ84H/G+IsIC+VlBfz+URfBC8qi/ypYvTls/VeF\n",
       "vB3dGQTpRfGaHYT9acCCGP5yrO85sY5OEATGVIJwWhbvTUe82vwYdyVBAB2PZT473puDBGF4UUxn\n",
       "JalAu4cgBNtjPXYThPA24OfAU/G6zxOE47tJFcihWFe/TRBY9wA9MY1nCMpnW7z2QcK974753xvj\n",
       "XkJQ4C/GvJ4P/Fq85u3AQuCaWM6LMmn2x+vPJTwjT8dzkzpqzdwNA+4iPLMdMa9zY5yDsf5bYrzk\n",
       "uXwpnvszgtJKnuMOwgKMx2LcuaTK5mLSd2d//H+c8Lx3xLIniuO8WP75MT+zYlgLaWPhBPAG4A9j\n",
       "vPnxf7Ly7OSjHorBS4lkZrdktuU1ztM4sH9dECYDcX8B4QE8QSrMXyY8vIlygPByHyNthR4h3LaX\n",
       "CS9o0gOZR3g5WggvTFLNv0x4Qa6J6X6U8BIOAMcG4cPAh4/C0T+Do28JAvXDA/Dhl6H/z9z9uiKC\n",
       "dyOkrXlo+S5cSXih7yF855l4vURRJIrFCQrtCuCDMexdwL2DcHgnfOiHhdcZjeK9isMfgCODoTU8\n",
       "nSAQD8e6+Skh/FcJgu/nMaX5sQ4PEITot2P9XhTrfiWhTbMB+G+Z+7eM0Lo/Fs/9NuGe3kNotR8E\n",
       "VhB6gfsJQqs/1sP58d46Ib+HPNTJBoKyTXp7y4B/IfQWjsZy/Jxw798S890d7+vBWMYNpG2utwFr\n",
       "47UGCEp/EUFYA3wSuI+gtDcDp8c0Tyc8P78T4x0irIB7drzGAUIj4yVOdi754xh2gvBcHAdeT77n\n",
       "cIS0x5YoPyN9ji3WzdKYzquA34z38dvA7xKE/FGCMp1CUMAXxXrxTH6IdTOVVGEeJLRJk7onXu/L\n",
       "cTufiYCZLc/KyqolXAcb2FLg0cz+Ggoc0DShjyGWa20YeZMdbbLEU9t1dgRL4hzNjqZpzcRLjiX+\n",
       "iS4PTrusbyGJc7rnhxB2O7ChiuXKzHZO7MGzDgEb05FLxRzKvTGsNkuChPpuP5E6vhMHZKtD52CY\n",
       "pTzjhTCbt3MgjMSa+lKow2TyWuJEX+Z5Z+nJeREb0vvQ4amDOHFwJ/vZ4ZwdJ9JJckPG6McZ28nQ\n",
       "3/bB/ES6pfE6XbtDvGm7h86Cbt8dnrPOE2FrGyy4lod892yKwzQH8/dj2u4wem36oXAfO1+CruNp\n",
       "2Tvj/Z36Qkh/+iBMPwGWuU4ywCI7gzoZYJA8v8mWPLPJ8/qqzP/7PD/UN3kXTs41ORbmm2T9c8kw\n",
       "3+meDgrIbolvLpnLMSWT/sk6Ol6LZ3IcZIxXJZ06ZHwq8B8E5/M0JonzuaB8mZd/Vn94EacfCS/Y\n",
       "rEHoPJK+3K0nouPtRBBi2Re+80j+/FmDMP1YDD8WhFxy/vQTMOtIHKpa0izV8stU2uS74eLWrq6L\n",
       "r/kzwjlrQz2FuhotDXJrQ83ckcYrXKah/HpI47TtyAw1Xls8zkjLRrTtCM/ErP5yzy/nnpVYdxui\n",
       "4hsIs7OnD8ZnNO4nz2trHE4963hoZCTLXLTvyN6H9H2afig+54OxYbI2f6zzRDjeeSIdjp1MeGw9\n",
       "kjYWunZPRKUQ69+rkY7FxMYVM3sLcCeh/3evu/9FwXF395G8pUIIIQqoluysi2IYDSkGIYQon2rJ\n",
       "zno4n4UQQjQwUgxCCCFySDEIIYTIIcUghBAihxSDEEKIHFIMQgghckgxCCGEyCHFIIQQIocUgxBC\n",
       "iBxSDEIIIXJIMQghhMghxSCEECKHFIMQQogcUgxCCCFySDEIIYTIIcUghBAihxSDEEKIHFIMQggh\n",
       "ckgxCCGEyCHFIIQQIocUgxBCiBxSDEIIIXKMWTGY2V+Z2VNm9iMz+4qZzcwcW2NmO8xsu5ldmgm/\n",
       "0My2xmOfrjTzQgghqk8lPYZNwGvd/XXA08AaADNbAlwDLAEuA+4yM4vnfAZ4j7svBhab2WUVXH/C\n",
       "YmbL652HWtHMZQOVb6LT7OWrFmNWDO6+2d0H4+73gAXx/xXAg+4+4O7PADuBi8zsVOAUd38sxvs8\n",
       "cOVYrz/BWV7vDNSQ5fXOQI1ZXu8M1Jjl9c5AjVle7wxMBKrlY7ge+Hr8Px/YlTm2CzitSPjuGC6E\n",
       "EKKBmDrSQTPbDMwrcmitu38txvkz4Ji7P1CD/AkhhBhnzN3HfrLZHwA3AG929yMx7GYAd/9k3H8U\n",
       "+DjwM+Cf3P3cGP57wMXu/v4i6Y49U0IIMYlxdxs91siM2GMYieg4/hOCcD+SOfQI8ICZrSOYihYD\n",
       "j7m7m1m/mV0EPAa8G1hfLO1qFEwIIcTYGHOPwcx2ANOA/THo39z9xnhsLcHvcBz4oLtvjOEXAvcB\n",
       "M4Cvu/uqinIvhBCi6lRkShJCCNF81HXm82SbJGdml8Xy7DCzj9Q7P2PBzBaa2T+Z2ZNm9u9mtiqG\n",
       "95jZZjN72sw2mVl35pyi97JRMbMpZva4mSUDLJqpbN1m9qX43m0zs4uarHxr4rO51cweMLO2iVw+\n",
       "M/ucme01s62ZsLLLU7bcdPe6bcAKoCX+/yTwyfh/CfAE0AqcSZgLkfRuHgPeGP9/HbisnmUoo6xT\n",
       "YjnOjOV6Aji33vkaQznmAa+P/zuBnwDnAn8J/GkM/8go97Kl3uUYpYyrgb8DHon7zVS2+4Hr4/+p\n",
       "wMxmKV/M40+Btrj/ReDaiVw+4DeBNwBbM2HllGdMcrOuPQafXJPk3gjsdPdn3H0A+HtCOScU7r7H\n",
       "3Z+I/w8CTxEGGVxOEDrE3+S+FLuXbxzXTJeBmS0A3gr8DZAMgmiWss0EftPdPwfg7sfd/SWapHxA\n",
       "PzAAtJvZVKAdeI4JXD53/2fgxYLgcsozJrnZSIvoNfskudOAZzP7SZkmLGZ2JqE18z1grrvvjYf2\n",
       "AnPj/+HuZaPyKcJou8FMWLOUbRHwCzPbYGY/NLN7zKyDJimfu+8H7gB+TlAIB9x9M01Svgzllqds\n",
       "uVlzxRBtYVuLbG/LxJkMk+SaystvZp3Alwmjzl7OHvPQXx2pvA1ZF2b2u8Dz7v44aW8hx0QtW2Qq\n",
       "cAFwl7tfABwCbs5GmMjlM7PXAB8imFHmA51m9q5snIlcvmKUUJ4xMeZ5DKXi7itGOh4nyb0VeHMm\n",
       "eDewMLO/gKDxdpOam5Lw3VXJaO0pLNNC8lp8wmBmrQSl8AV3fzgG7zWzee6+J3Zdn4/hxe5lo96z\n",
       "3wAuN7O3AtOBLjP7As1RNgjP2y53/37c/xJh8cs9TVK+XwX+1d33AZjZV4Bfp3nKl1DO8zgmuVnv\n",
       "UUnJJLkrfOgkuXea2TQzW0Q6SW4P0B9HUhhhktzDQxJuTH5AWFH2TDObRliB9pE656lsYr3fC2xz\n",
       "9zszhx4hOPqIvw9nwofcy/HKbzm4+1p3X+jui4B3At9y93fTBGWD4B8CnjWzs2PQJcCTwNdogvIB\n",
       "24GlZjYjPqeXANtonvIllPU8jklu1tnjvoOwVMbjcbsrc2wtwXmyHViZCb8Q2BqPra9n/sdQ3rcQ\n",
       "RvHsBNbUOz9jLMObCPb3JzL37TKgB/gGYQn2TUD3aPeykTfgYtJRSU1TNuB1wPeBHwFfIYxKaqby\n",
       "/SlB2W0lOGZbJ3L5gAcJ/pJjBB/ldWMpT7lyUxPchBBC5GikUUlCCCEaACkGIYQQOaQYhBBC5JBi\n",
       "EEIIkUOKQQghRA4pBiGEEDmkGIQQQuSQYhBCCJHj/wOp8U8/jFRDQgAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a66a668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(thresholds)),thresholds[:,2])\n",
    "print sum(thresholds[:,2] >150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "s: [366808L, 356604L]\n",
      "w: [366808.0, 356604.0]\n",
      "s: [215748L, 151060L, 152692L, 203912L]\n",
      "w: [215748.0, 151060.0, 152692.0, 203912.0]\n",
      "s: [86061L, 117851L, 87762L, 64930L, 146051L, 69697L, 44793L, 106267L]\n",
      "w: [86061.0, 117851.0, 87762.0, 64930.0, 146051.0, 69697.0, 44793.0, 106267.0]\n",
      "s: [39990L, 77861L, 34621L, 30309L, 49411L, 38351L, 52749L, 33312L, 111614L, 34437L, 19036L, 50661L, 28327L, 77940L, 30932L, 13861L]\n",
      "w: [39990.0, 77861.0, 34621.0, 30309.0, 49411.0, 38351.0, 52749.0, 33312.0, 111614.0, 34437.0, 19036.0, 50661.0, 28327.0, 77940.0, 30932.0, 13861.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_functions.py:97: RuntimeWarning: divide by zero encountered in log\n",
      "  logs = np.array(map(np.log,distribution))\n"
     ]
    }
   ],
   "source": [
    "#get them...\n",
    "thresholds_active = thresholds[thresholds[:,2]>100] #at least 100 times used in the original ensemble\n",
    "print len(thresholds_active)\n",
    "criteria = hierarchy.select_criteria(trainFactory,thresholds_active,4,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   10. ,   408.5,  1072. ]),\n",
       " array([  29. ,    1.5,  134. ]),\n",
       " array([  13. ,    6.5,  139. ]),\n",
       " array([  12. ,    4.5,  101. ])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[380323L, 427433L, 333122L, 367226L, 402553L, 361880L, 390481L, 350453L, 436276L, 381647L, 346568L, 393100L, 362210L, 410689L, 369445L, 328576L]\n",
      "[380323.0, 427433.0, 333122.0, 367226.0, 402553.0, 361880.0, 390481.0, 350453.0, 436276.0, 381647.0, 346568.0, 393100.0, 362210.0, 410689.0, 369445.0, 328576.0]\n"
     ]
    }
   ],
   "source": [
    "split = hierarchy.split_upper(trainFactory,criteria,equalizeWeights=False,split_weights=1.,split_inclusion=.7) \n",
    "#при каждом разделении в подвыборку  попадает split_inclusion примеров из другой половины выборки с весом split_weights\n",
    "print [split[i].events.shape[0] for i in split]\n",
    "print [sum(split[i].weights) for i in split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hierarchical stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-41b2a9905dcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                  \u001b[0mregularizer\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.0004\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                                  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_joblib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                                  weights_outside_leaf = 0.75**.25, inclusion_outside_leaf = .5**.25) \n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Митинг\\Documents\\Python Scripts\\pruner\\hierarchy.py\u001b[0m in \u001b[0;36mtrain_splitted_boosts\u001b[1;34m(trees, factory, criteria, loss, learning_rate, breadth, nTrees_leaf, trees_sample_size, verbose, learning_rate_decay, trees_sample_increase, regularizer, weights_outside_leaf, inclusion_outside_leaf, use_joblib, joblib_method, use_joblib_inner, joblib_method_inner, copy_pred_inner, n_jobs, n_jobs_inner, initialTrees)\u001b[0m\n\u001b[0;32m    109\u001b[0m     factories = split_upper(factory,criteria,\n\u001b[0;32m    110\u001b[0m                                    \u001b[0msplit_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights_outside_leaf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                                    split_inclusion= inclusion_outside_leaf)\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mleaves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Митинг\\Documents\\Python Scripts\\pruner\\hierarchy.py\u001b[0m in \u001b[0;36msplit_upper\u001b[1;34m(factory, criteria, returnIndices, equalizeWeights, normalizeWeights, split_weights, split_inclusion)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msplt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mdichotomy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_events\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0msplit_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdichotomy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_inclusion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturnIndices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mindices_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdichotomy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Митинг\\Documents\\Python Scripts\\pruner\\factory.py\u001b[0m in \u001b[0;36msplit_by\u001b[1;34m(self, boolean, offclass_weights, offclass_sample)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mFactoryClass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         factory_p = FactoryClass(np.append(self.events[boolean],self.events[inverse][inverse_addition_indices],0),\n\u001b[0m\u001b[0;32m     91\u001b[0m                                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mboolean\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minverse_addition_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                                 np.append(self.weights[boolean],self.weights[inverse][inverse_addition_indices]*offclass_weights,0))\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\lib\\function_base.pyc\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3883\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3884\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "#note that it equalizes weights which might be suboptimal.\n",
    "trees_splitted = hierarchy.train_splitted_boosts(trees, trainFactory,criteria,\n",
    "                                                 loss = MSELoss,\n",
    "                                                 learning_rate = 0.25, \n",
    "                                                 nTrees_leaf= 150,\n",
    "                                                 trees_sample_size=200,\n",
    "                                                 regularizer =0.0004,\n",
    "                                                 verbose=True,use_joblib = False,n_jobs = -1,\n",
    "                                                 weights_outside_leaf = 0.75**.25, inclusion_outside_leaf = .5**.25) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_splitted= hierarchy.predict_splitted(testFactory,criteria,trees_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_test = testFactory.weights\n",
    "Yts = testFactory.labels\n",
    "print 'spltd\\t',metrics.mean_squared_error(Yts,y_pred_splitted)\n",
    "print 'greedy\\t',metrics.mean_squared_error(Yts,y_pred_greedy),\n",
    "print 'stupid\\t',metrics.mean_squared_error(Yts,y_pred_stupid),\n",
    "print 'full\\t',metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#AUC learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I know it could have been done without quadratic complexity over trees, but...\n",
    "#btw equalizing 1-0 weights for the training set is not optimal in terms of AUC, but it is close to being so.\n",
    "n_trees =150\n",
    "auc_stupid_lcurve = [(0.5,)]\n",
    "auc_greedy_lcurve = [(0.5,)]\n",
    "auc_splitted_lcurve = [(0.5,)]\n",
    "for i in range(1,n_trees):\n",
    "    #stpd\n",
    "    pred = testFactory.predict(trees[:i])\n",
    "    auc = metrics.roc_auc_score(testFactory.labels,pred,sample_weight=testFactory.weights),\n",
    "    auc_stupid_lcurve.append( auc)    \n",
    "    #grdy\n",
    "    pred = testFactory.predict(res_greedy[:i])#res_greedy_wheel is not optimized for this dissection \n",
    "    auc = metrics.roc_auc_score(testFactory.labels,pred,sample_weight=testFactory.weights),\n",
    "    auc_greedy_lcurve.append( auc)\n",
    "    #split\n",
    "    trees_i = {code:trees_splitted[code][:i] for code in trees_splitted}\n",
    "    pred = hierarchy.predict_splitted(testFactory,criteria,trees_i)\n",
    "\n",
    "    auc = metrics.roc_auc_score(testFactory.labels,pred,sample_weight=testFactory.weights),\n",
    "    auc_splitted_lcurve.append( auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = range(1,n_trees)\n",
    "plt.figure(figsize = [14,14])\n",
    "plt.plot(p,[0.935834322801 for i in range(1,n_trees)],label = \"full\")\n",
    "plt.plot(p,auc_stupid_lcurve[1:n_trees],label = \"stupid\")\n",
    "plt.plot(p,auc_greedy_lcurve[1:n_trees],label = \"greedy\")\n",
    "plt.plot(p,auc_splitted_lcurve[1:n_trees],label = \"splitted\")\n",
    "plt.title('learning curves(AUC)')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LogLoss learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I know it could have been done without quadratic complexity over trees, but...\n",
    "#btw equalizing 1-0 weights for the training set is not optimal in terms of AUC, but it is close to being so.\n",
    "n_trees =150\n",
    "logloss_stupid_lcurve = [(0.5,)]\n",
    "logloss_greedy_lcurve = [(0.5,)]\n",
    "logloss_splitted_lcurve = [(0.5,)]\n",
    "for i in range(1,n_trees):\n",
    "    #stpd\n",
    "    pred = testFactory.predict(trees[:i])\n",
    "    logloss = LogLoss.score(testFactory,pred),\n",
    "    logloss_stupid_lcurve.append( logloss)    \n",
    "    #grdy\n",
    "    pred = testFactory.predict(res_greedy[:i])#res_greedy_wheel is not optimized for this dissection \n",
    "    logloss = LogLoss.score(testFactory,pred),\n",
    "    logloss_greedy_lcurve.append(logloss)\n",
    "    #split\n",
    "    trees_i = {code:trees_splitted[code][:i] for code in trees_splitted}\n",
    "    pred = hierarchy.predict_splitted(testFactory,criteria,trees_i)\n",
    "\n",
    "    logloss = LogLoss.score(testFactory,pred),\n",
    "    logloss_splitted_lcurve.append(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = range(1,n_trees)\n",
    "plt.figure(figsize = [14,14])\n",
    "plt.plot(p,[0.935834322801 for i in range(1,n_trees)],label = \"full\")\n",
    "plt.plot(p,logloss_stupid_lcurve[1:n_trees],label = \"stupid\")\n",
    "plt.plot(p,logloss_lcurve[1:n_trees],label = \"greedy\")\n",
    "plt.plot(p,logloss_splitted_lcurve[1:n_trees],label = \"splitted\")\n",
    "plt.title('learning curves(LogLoss)')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
