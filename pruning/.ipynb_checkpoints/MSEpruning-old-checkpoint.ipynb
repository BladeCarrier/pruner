{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#math & plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sklearn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#debug purposes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "target_n_trees = 100\n",
    "global_n_jobs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "def cDump(obj,fname):\n",
    "    with open(fname,'w') as f:\n",
    "        cPickle.dump(obj,f)\n",
    "def cLoad(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        return cPickle.load(f)\n",
    "        \n",
    "from StringIO import StringIO\n",
    "import _matrixnetapplier as mnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997 == 9997\n"
     ]
    }
   ],
   "source": [
    "with open('../formula/MSLR10k_ef.mx', 'r') as f:\n",
    "    formula = mnet.MatrixnetClassifier(StringIO(cPickle.load(f))) #btw he's a regressor, not classifier\n",
    "\n",
    "depth, nTrees, itr = formula.iterate_trees().next()\n",
    "trees = [tree for tree in itr]\n",
    "print len(trees), '==',nTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import h5py. H5 IO operations will be unavailable\n"
     ]
    }
   ],
   "source": [
    "import io_ranking as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted that\n",
      "CPU times: user 22 µs, sys: 8 µs, total: 30 µs\n",
      "Wall time: 33.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##warning! this can take a long time. no need to rerun that code if u have CSV files created once.\n",
    "#io.save_csv(\"../data/MSLR10/Fold1/train.txt\",\"../data/MSLR10/mslr_train\")\n",
    "#io.save_csv(\"../data/MSLR10/Fold1/test.txt\",\"../data/MSLR10/mslr_test\")\n",
    "#io.save_csv(\"../data/MSLR10/Fold1/vali.txt\",\"../data/MSLR10/mslr_vali\")\n",
    "print \"converted that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from ../data/MSLR10/mslr_train\n",
      "done\n",
      "reading from ../data/MSLR10/mslr_test\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load training set\n",
    "Xtr,Qtr,Ytr = io.load_csv(\"../data/MSLR10/mslr_train\")\n",
    "Xts,Qts,Yts = io.load_csv(\"../data/MSLR10/mslr_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from factory import RegressionFactory\n",
    "#DataFactory is just a data wrapper that can handle splits, predictions, etc. \n",
    "#Used to avoid recomputing metadata at each predictions and passing large argument strings\n",
    "trainFactory = RegressionFactory(Xtr,Ytr)\n",
    "testFactory = RegressionFactory(Xts,Yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xtr,Ytr,Xts,Yts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainFactory.labels = trainFactory.labels.astype('int8')\n",
    "trainFactory.events = trainFactory.events.astype('float32')\n",
    "trainFactory.weights = trainFactory.weights.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (723412, 136) qids: 6000\n",
      "test:  (241521, 136) qids: 2000\n",
      "qid intersection: 0 (must be 0)\n"
     ]
    }
   ],
   "source": [
    "print \"train: \",trainFactory.events.shape,\"qids:\",len(set(Qtr))\n",
    "print \"test: \",testFactory.events.shape,\"qids:\",len(set(Qts))\n",
    "print \"qid intersection:\",len(set.intersection(set(Qtr),set(Qts))),\"(must be 0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy pruning for the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import greedy\n",
    "from loss_functions import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  459602.030019\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  443630.409199 \n",
      "last loss =  443630.409199\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  434218.005318 \n",
      "last loss =  434218.005318\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  427618.286301 \n",
      "last loss =  427618.286301\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  423206.13052 \n",
      "last loss =  423206.13052\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  419525.591279 \n",
      "last loss =  419525.591279\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  417288.020333 \n",
      "last loss =  417288.020333\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  415310.265513 \n",
      "last loss =  415310.265513\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  413709.949707 \n",
      "last loss =  413709.949707\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  412242.465244 \n",
      "last loss =  412242.465244\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  411152.338942 \n",
      "last loss =  411152.338942\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  410101.408111 \n",
      "last loss =  410101.408111\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  409065.328775 \n",
      "last loss =  409065.328775\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  408130.79792 \n",
      "last loss =  408130.79792\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  407250.182017 \n",
      "last loss =  407250.182017\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  406491.413712 \n",
      "last loss =  406491.413712\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  405795.496368 \n",
      "last loss =  405795.496368\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  405180.104308 \n",
      "last loss =  405180.104308\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  404575.908949 \n",
      "last loss =  404575.908949\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  404061.804249 \n",
      "last loss =  404061.804249\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  403515.419162 \n",
      "last loss =  403515.419162\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  403049.324429 \n",
      "last loss =  403049.324429\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  402608.387027 \n",
      "last loss =  402608.387027\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  402152.995966 \n",
      "last loss =  402152.995966\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  401771.148939 \n",
      "last loss =  401771.148939\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  401337.476644 \n",
      "last loss =  401337.476644\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  400957.383166 \n",
      "last loss =  400957.383166\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  400619.212817 \n",
      "last loss =  400619.212817\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  400240.614819 \n",
      "last loss =  400240.614819\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  399877.765763 \n",
      "last loss =  399877.765763\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  399544.11757 \n",
      "last loss =  399544.11757\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  399213.113492 \n",
      "last loss =  399213.113492\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  398905.701493 \n",
      "last loss =  398905.701493\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  398593.013146 \n",
      "last loss =  398593.013146\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  398263.434069 \n",
      "last loss =  398263.434069\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  397956.32553 \n",
      "last loss =  397956.32553\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  397693.608994 \n",
      "last loss =  397693.608994\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  397410.062248 \n",
      "last loss =  397410.062248\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  397134.134054 \n",
      "last loss =  397134.134054\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  396862.131191 \n",
      "last loss =  396862.131191\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  396593.005253 \n",
      "last loss =  396593.005253\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  396314.827802 \n",
      "last loss =  396314.827802\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  396069.577837 \n",
      "last loss =  396069.577837\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  395811.141792 \n",
      "last loss =  395811.141792\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  395567.175476 \n",
      "last loss =  395567.175476\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  395315.971526 \n",
      "last loss =  395315.971526\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  395084.987053 \n",
      "last loss =  395084.987053\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  394856.777166 \n",
      "last loss =  394856.777166\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  394621.042672 \n",
      "last loss =  394621.042672\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  394371.635568 \n",
      "last loss =  394371.635568\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  394149.252124 \n",
      "last loss =  394149.252124\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  393927.12139 \n",
      "last loss =  393927.12139\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  393711.219776 \n",
      "last loss =  393711.219776\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  393482.181299 \n",
      "last loss =  393482.181299\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  393285.218278 \n",
      "last loss =  393285.218278\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  393078.748917 \n",
      "last loss =  393078.748917\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  392877.347506 \n",
      "last loss =  392877.347506\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  392672.084192 \n",
      "last loss =  392672.084192\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  392479.642052 \n",
      "last loss =  392479.642052\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  392268.245476 \n",
      "last loss =  392268.245476\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  392080.219509 \n",
      "last loss =  392080.219509\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  391898.497993 \n",
      "last loss =  391898.497993\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  391722.405783 \n",
      "last loss =  391722.405783\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  391537.500793 \n",
      "last loss =  391537.500793\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  391357.074157 \n",
      "last loss =  391357.074157\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  391186.266592 \n",
      "last loss =  391186.266592\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  391021.06268 \n",
      "last loss =  391021.06268\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  390829.173097 \n",
      "last loss =  390829.173097\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  390664.092473 \n",
      "last loss =  390664.092473\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  390489.383526 \n",
      "last loss =  390489.383526\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  390318.112685 \n",
      "last loss =  390318.112685\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  390155.560763 \n",
      "last loss =  390155.560763\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  389989.458714 \n",
      "last loss =  389989.458714\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  389838.106634 \n",
      "last loss =  389838.106634\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  389684.916129 \n",
      "last loss =  389684.916129\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  389524.816229 \n",
      "last loss =  389524.816229\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  389365.997656 \n",
      "last loss =  389365.997656\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  389204.673187 \n",
      "last loss =  389204.673187\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  389057.145955 \n",
      "last loss =  389057.145955\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  388906.353509 \n",
      "last loss =  388906.353509\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  388749.756278 \n",
      "last loss =  388749.756278\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  388603.535482 \n",
      "last loss =  388603.535482\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  388452.143297 \n",
      "last loss =  388452.143297\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  388312.089781 \n",
      "last loss =  388312.089781\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  388175.141433 \n",
      "last loss =  388175.141433\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  388033.539324 \n",
      "last loss =  388033.539324\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  387893.240801 \n",
      "last loss =  387893.240801\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  387745.880117 \n",
      "last loss =  387745.880117\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  387612.776056 \n",
      "last loss =  387612.776056\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  387467.38198 \n",
      "last loss =  387467.38198\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  387330.86614 \n",
      "last loss =  387330.86614\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  387197.509846 \n",
      "last loss =  387197.509846\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  387068.653433 \n",
      "last loss =  387068.653433\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  386919.563176 \n",
      "last loss =  386919.563176\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  386784.992424 \n",
      "last loss =  386784.992424\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  386655.057163 \n",
      "last loss =  386655.057163\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  386529.739233 \n",
      "last loss =  386529.739233\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  386406.634194 \n",
      "last loss =  386406.634194\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  386280.112319 \n",
      "last loss =  386280.112319\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  386158.223759 \n",
      "last loss =  386158.223759\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "CPU times: user 46min 6s, sys: 6min 31s, total: 52min 37s\n",
      "Wall time: 11min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "read = False\n",
    "fname=\"../dumps/last_greedy.pcl\"\n",
    "if not read:\n",
    "    trees_greedy = greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                              loss = MSELoss,\n",
    "                                              learning_rate = .35,\n",
    "                                              learning_rate_decay=1.,# no decay\n",
    "                                              nTrees = target_n_trees,\n",
    "                                              trees_sample_size =500, #chosen from the ensemble at random each iteration\n",
    "                                              verbose = True,\n",
    "                                              regularizer=0.0005*len(trainFactory.labels), #added to gradient walker's leaf denominator\n",
    "                                              use_joblib=True,\n",
    "                                              n_jobs=global_n_jobs, #threading by default\n",
    "                                              )\n",
    "    cDump(trees_greedy,fname)\n",
    "else:\n",
    "    trees_greedy = cLoad(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from greedy import PrunedFormula as pf\n",
    "bias = 0.0# NOT np.average(trainFactory.labels)\n",
    "trees_stupid = pf(trees[:target_n_trees],bias)\n",
    "trees_full = pf(trees,bias)\n",
    "\n",
    "y_pred_stupid = trees_stupid.predict(testFactory)\n",
    "y_pred_full = trees_full.predict(testFactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_greedy = trees_greedy.predict(testFactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56464797177 0.695731252371 0.554568902063\n",
      "well...\n"
     ]
    }
   ],
   "source": [
    "print metrics.mean_squared_error(testFactory.labels,y_pred_greedy),\n",
    "print metrics.mean_squared_error(testFactory.labels,y_pred_stupid),\n",
    "print metrics.mean_squared_error(testFactory.labels,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723412\n"
     ]
    }
   ],
   "source": [
    "print len(trainFactory.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  459156.946399\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  444456.579169 \n",
      "last loss =  444456.579169\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  434239.164414 \n",
      "last loss =  434239.164414\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  426912.700286 \n",
      "last loss =  426912.700286\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  421682.548343 \n",
      "last loss =  421682.548343\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  418374.443641 \n",
      "last loss =  418374.443641\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  415902.563383 \n",
      "last loss =  415902.563383\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  413818.393533 \n",
      "last loss =  413818.393533\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  412038.215145 \n",
      "last loss =  412038.215145\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  410474.080895 \n",
      "last loss =  410474.080895\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  409201.529505 \n",
      "last loss =  409201.529505\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  408078.858154 \n",
      "last loss =  408078.858154\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  407160.785221 \n",
      "last loss =  407160.785221\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  406213.727583 \n",
      "last loss =  406213.727583\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  405389.586693 \n",
      "last loss =  405389.586693\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  404526.35266 \n",
      "last loss =  404526.35266\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  403810.638307 \n",
      "last loss =  403810.638307\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  403159.752003 \n",
      "last loss =  403159.752003\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  402538.415913 \n",
      "last loss =  402538.415913\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  401943.35095 \n",
      "last loss =  401943.35095\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  401389.080877 \n",
      "last loss =  401389.080877\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  400805.218915 \n",
      "last loss =  400805.218915\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  400197.723128 \n",
      "last loss =  400197.723128\n",
      "learning_rate =  0.35\n",
      "sample_size 500\n",
      "\n",
      "iteration #"
     ]
    }
   ],
   "source": [
    "read = False\n",
    "fname = \"../dumps/last_grid2.pcl\"\n",
    "if not read:\n",
    "    trees_grid = {}\n",
    "    for lr in [0.35,0.4,0.5]:\n",
    "            for r in [0,0.001]:\n",
    "                _formula = greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                                      loss = MSELoss,\n",
    "                                                      learning_rate = lr,\n",
    "                                                      learning_rate_decay=1.,# no decay\n",
    "                                                      nTrees = target_n_trees,\n",
    "                                                      trees_sample_size =500, #chosen from the ensemble at random each iteration\n",
    "                                                      verbose = True,\n",
    "                                                      regularizer=r*len(trainFactory.labels), #added to gradient walker's leaf denominator\n",
    "                                                      use_joblib=True,\n",
    "                                                      n_jobs=global_n_jobs, #threading by default\n",
    "                                                      )\n",
    "                trees_grid[(lr,r)] = _formula\n",
    "                \n",
    "    cDump(trees_grid, fname)\n",
    "else:\n",
    "    trees_grid = cLoad(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for key in trees_grid:\n",
    "    _formula = trees_grid[key]\n",
    "    y_pred_sanity = _formula.predict(testFactory)\n",
    "    print key, metrics.mean_squared_error(testFactory.labels,y_pred_sanity)\n",
    "    res.append((key, metrics.mean_squared_error(testFactory.labels,y_pred_sanity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usability distribution\n",
    "read = True\n",
    "fname = '../dumps/thresholds.pcl'\n",
    "if not read:\n",
    "    thresholds = mnet.get_thresholds(trees,formula.feature_ids,0.001)\n",
    "    #todo: make a LOGICALLY CONSISTENT parallel threshold extractor AND criteria selector. Right now joblib slows things down\n",
    "    cDump(thresholds,fname)\n",
    "else:    \n",
    "    thresholds = cLoad(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(thresholds)),thresholds[:,2])\n",
    "print sum(thresholds[:,2] >150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usability distribution\n",
    "read = False\n",
    "fname = '../dumps/criteria.pcl'\n",
    "if not read:\n",
    "    thresholds_active = thresholds[thresholds[:,2]>100] #at least 100 times used in the original ensemble\n",
    "    print len(thresholds_active)\n",
    "    criteria = hierarchy.select_criteria(trainFactory,thresholds_active,4,True)\n",
    "    cDump(criteria,fname)\n",
    "else:\n",
    "    criteria = cLoad(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split = hierarchy.split_upper(trainFactory,criteria,equalizeWeights=False,split_weights=1.,split_inclusion=.7) \n",
    "#при каждом разделении в подвыборку  попадает split_inclusion примеров из другой половины выборки с весом split_weights\n",
    "#print [split[i].events.shape[0] for i in split]\n",
    "#print [sum(split[i].weights) for i in split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hierarchical stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "read = False\n",
    "fname = \"../dumps/last_hierarchy.pcl\"\n",
    "if not read:\n",
    "    \n",
    "    trees_splitted = hierarchy.train_splitted_boosts(trees, trainFactory,criteria,\n",
    "                                                     breadth = 1,\n",
    "                                                     loss = MSELoss,\n",
    "                                                     learning_rate = 0.25, \n",
    "                                                     nTrees_leaf= target_n_trees,\n",
    "                                                     trees_sample_size=500,\n",
    "                                                     regularizer =0.0001,\n",
    "                                                     verbose=True,\n",
    "                                                     use_joblib = True,n_jobs = global_n_jobs,\n",
    "                                                     joblib_backend = \"multiprocessing\",\n",
    "                                                     weights_outside_leaf = 0.75**.25, inclusion_outside_leaf = 0.1*.25) \n",
    "    cDump(trees_splitted,fname)\n",
    "else:\n",
    "    trees_splitted = cLoad(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_splitted= trees_splitted.predict(testFactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_test = testFactory.weights\n",
    "Yts = testFactory.labels\n",
    "print 'spltd\\t',metrics.mean_squared_error(Yts,y_pred_splitted)\n",
    "print 'greedy\\t',metrics.mean_squared_error(Yts,y_pred_greedy)\n",
    "print 'stupid\\t',metrics.mean_squared_error(Yts,y_pred_stupid)\n",
    "print 'full\\t',metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read = True\n",
    "fname = \"../dumps/last_hierarchy_grid.pcl\"\n",
    "if not read:\n",
    "    splt_grid = {}\n",
    "    for lr in [0.2,0.25,0.3]:\n",
    "            for sh in [0.05,0.2,0.5]:\n",
    "                _formula = hierarchy.train_splitted_boosts(trees, trainFactory,criteria,\n",
    "                                                     breadth = 1,\n",
    "                                                     loss = MSELoss,\n",
    "                                                     learning_rate = 0.25, \n",
    "                                                     nTrees_leaf= target_n_trees,\n",
    "                                                     trees_sample_size=500,\n",
    "                                                     regularizer =0.0001,\n",
    "                                                     verbose=True,\n",
    "                                                     use_joblib = True,n_jobs = global_n_jobs,\n",
    "                                                     joblib_backend = \"multiprocessing\",\n",
    "                                                     weights_outside_leaf = 0.75**.25, inclusion_outside_leaf = sh*.25) \n",
    "                trees_grid[(lr,sh)] = _formula\n",
    "                \n",
    "    cDump(splt_grid, fname)\n",
    "else:\n",
    "    splt_grid = cLoad(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'splt_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8223128752c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplt_grid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0m_formula\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplt_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0my_pred_sanity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_formula\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFactory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFactory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_sanity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'splt_grid' is not defined"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for key in splt_grid:\n",
    "    _formula = splt_grid[key]\n",
    "    y_pred_sanity = _formula.predict(testFactory)\n",
    "    print key, metrics.mean_squared_error(testFactory.labels,y_pred_sanity)\n",
    "    res.append((key, metrics.mean_squared_error(testFactory.labels,y_pred_sanity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MSE learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_curve(formula,factory, metric,n_points = None):\n",
    "    \n",
    "    lcurve = []\n",
    "\n",
    "    Ypred = np.zeros(len(factory.labels))\n",
    "                  \n",
    "    for i,tree_pred in enumerate(formula.staged_predict(factory)):\n",
    "\n",
    "        Ypred += tree_pred\n",
    "        lcurve.append(metric(factory.labels, Ypred,sample_weight = factory.weights))\n",
    "        if n_points is not None and i >= n_points:\n",
    "            break\n",
    "    while n_points is not None and i < n_points:\n",
    "        i+=1\n",
    "        lcurve.append(lcurve[-1])\n",
    "        \n",
    "    return lcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metric_name = 'MSE'\n",
    "n_trees = target_n_trees\n",
    "metric = metrics.mean_squared_error\n",
    "\n",
    "stupid_lcurve = learning_curve(trees_stupid,testFactory,metric,n_trees)\n",
    "greedy_lcurve = learning_curve(trees_greedy,testFactory,metric,n_trees)\n",
    "splitted_lcurve = learning_curve(trees_splitted,testFactory,metric,n_trees)\n",
    "\n",
    "full_line = metric(testFactory.labels,y_pred_full)\n",
    "\n",
    "p = range(n_trees+1)\n",
    "\n",
    "plt.figure(figsize = [14,14])\n",
    "plt.plot(p,[full_line for i in p],label = \"full\")\n",
    "plt.plot(p,stupid_lcurve,label = \"stupid\")\n",
    "plt.plot(p,greedy_lcurve,label = \"greedy\")\n",
    "plt.plot(p,splitted_lcurve,label = \"splitted\")\n",
    "plt.title('learning curves('+metric_name+')')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
