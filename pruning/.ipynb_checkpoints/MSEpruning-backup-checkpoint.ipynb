{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#general\n",
    "import copy\n",
    "import random\n",
    "\n",
    "#math & plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "#sklearn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "#EF applier\n",
    "import _matrixnetapplier as mnet\n",
    "\n",
    "#persistance\n",
    "import cPickle\n",
    "from StringIO import StringIO\n",
    "\n",
    "#debug purposes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997 == 9997\n"
     ]
    }
   ],
   "source": [
    "with open('../formula/MSLR10k_ef.mx', 'r') as f:\n",
    "    formula = mnet.MatrixnetClassifier(StringIO(cPickle.load(f))) #btw he's a regressor, not classifier\n",
    "\n",
    "depth, nTrees, itr = formula.iterate_trees().next()\n",
    "trees = [tree for tree in itr]\n",
    "print len(trees), '==',nTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_as_h5(path_to_txt,output_name=\"mslr\"):\n",
    "    \n",
    "    print \"opening \"+path_to_txt\n",
    "    f = open(path_to_txt)\n",
    "    labels = []\n",
    "    features = []\n",
    "    print \"extracting...\"\n",
    "    for line in f:\n",
    "        line = line[:line.find('#') - 1]#удалить комменты из конца линии\n",
    "        ls = line.split()\n",
    "        labels.append(int(ls[0]))\n",
    "        features.append([float(x[x.find(':') + 1:]) for x in ls[1:]])\n",
    "    f.close()\n",
    "    print \"converting & sorting...\"\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    features = np.asarray(features)\n",
    "    query = features[:, 0].astype(int)\n",
    "    features = features[:, 1:]\n",
    "    sorter = np.argsort(query)\n",
    "    query,labels,features = query[sorter],labels[sorter],features[sorter]\n",
    "    print \"saving...\"\n",
    "    h5f = h5py.File(output_name, 'w')\n",
    "    h5f.create_dataset('qids', data=query)\n",
    "    h5f.create_dataset('labels', data=labels)\n",
    "    h5f.create_dataset('features', data=features)\n",
    "    h5f.close()\n",
    "    print \"done\"\n",
    "    return features,query,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_csv(path_to_txt,output_name = \"mslr\"):\n",
    "    print \"opening \"+path_to_txt\n",
    "    f = open(path_to_txt)\n",
    "    labels = []\n",
    "    features = []\n",
    "    print \"extracting...\"\n",
    "    for line in f:\n",
    "        line = line[:line.find('#') - 1]#удалить комменты из конца линии\n",
    "        ls = line.split()\n",
    "        labels.append(int(ls[0]))\n",
    "        features.append([float(x[x.find(':') + 1:]) for x in ls[1:]])\n",
    "    f.close()\n",
    "    print \"converting & sorting...\"\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    features = np.asarray(features)\n",
    "    query = features[:, 0].astype(int)\n",
    "    features = features[:, 1:]\n",
    "    sorter = np.argsort(query)\n",
    "    query,labels,features = query[sorter],labels[sorter],features[sorter]\n",
    "    print \"saving...\"\n",
    "    np.savetxt(output_name+\".qids.csv\",query,delimiter=',')\n",
    "    np.savetxt(output_name+\".lavels.csv\",labels,delimiter=',')\n",
    "    np.savetxt(output_name+\".features.csv\",features,delimiter=',')\n",
    "    print \"done\"\n",
    "    return features,query,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted that\n",
      "CPU times: user 32 µs, sys: 7 µs, total: 39 µs\n",
      "Wall time: 42 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##warning! this can take a long time. no need to rerun that code if u have CSV files created once.\n",
    "#save_csv(\"../data/MSLR10/Fold1/train.txt\",\"../data/MSLR10/mslr_train\")\n",
    "#save_csv(\"../data/MSLR10/Fold1/test.txt\",\"../data/MSLR10/mslr_test\")\n",
    "#save_csv(\"../data/MSLR10/Fold1/vali.txt\",\"../data/MSLR10/mslr_vali\")\n",
    "print \"converted that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load training set\n",
    "def load_h5(name):\n",
    "    print \"reading from\",name\n",
    "    h5f = h5py.File(name,'r')\n",
    "    labels = h5f['labels'][:]\n",
    "    qids = h5f['qids'][:]\n",
    "    features = h5f['features'][:]\n",
    "    h5f.close()\n",
    "    print \"done\"\n",
    "    return features, qids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(name):\n",
    "    print \"reading from\",name\n",
    "    qids = np.loadtxt(name+\".qids.csv\",delimiter=',')\n",
    "    labels = np.loadtxt(name+\".lavels.csv\",delimiter=',')\n",
    "    features = np.loadtxt(name+\".features.csv\",delimiter=',')\n",
    "    print \"done\"\n",
    "    return features, qids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from ../data/MSLR10/mslr_train\n",
      "done\n",
      "reading from ../data/MSLR10/mslr_test\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "Xtr,Qtr,Ytr = load_csv(\"../data/MSLR10/mslr_train\")\n",
    "Xts,Qts,Yts = load_csv(\"../data/MSLR10/mslr_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from factory import RegressionFactory\n",
    "#s_ind = np.arange(1000)\n",
    "#trainFactory = RegressionFactory(Xtr[s_ind],Ytr[s_ind])\n",
    "#DataFactory is just a data wrapper that can handle splits, predictions, etc. \n",
    "#Used to avoid recomputing metadata at each predictions and passing large argument strings\n",
    "trainFactory = RegressionFactory(Xtr,Ytr)\n",
    "testFactory = RegressionFactory(Xts,Yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (723412, 136) qids: 6000\n",
      "test:  (241521, 136) qids: 2000\n",
      "qid intersection: 0 (must be 0)\n"
     ]
    }
   ],
   "source": [
    "print \"train: \",Xtr.shape,\"qids:\",len(set(Qtr))\n",
    "print \"test: \",Xts.shape,\"qids:\",len(set(Qts))\n",
    "print \"qid intersection:\",len(set.intersection(set(Qtr),set(Qts))),\"(must be 0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy pruning for the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import greedy\n",
    "from loss_functions import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration # 0  ntrees =  1 \n",
      "best loss =  806973.9353\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 1  ntrees =  2 \n",
      "best loss =  804820.873461 \n",
      "last loss =  804820.873461\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 2  ntrees =  3 \n",
      "best loss =  802672.885476 \n",
      "last loss =  802672.885476\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 3  ntrees =  4 \n",
      "best loss =  800541.538417 \n",
      "last loss =  800541.538417\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 4  ntrees =  5 \n",
      "best loss =  798413.542072 \n",
      "last loss =  798413.542072\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 5  ntrees =  6 \n",
      "best loss =  796292.841919 \n",
      "last loss =  796292.841919\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 6  ntrees =  7 \n",
      "best loss =  794202.781225 \n",
      "last loss =  794202.781225\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 7  ntrees =  8 \n",
      "best loss =  792111.511678 \n",
      "last loss =  792111.511678\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 8  ntrees =  9 \n",
      "best loss =  790049.895213 \n",
      "last loss =  790049.895213\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 9  ntrees =  10 \n",
      "best loss =  787995.347469 \n",
      "last loss =  787995.347469\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 10  ntrees =  11 \n",
      "best loss =  785937.258821 \n",
      "last loss =  785937.258821\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 11  ntrees =  12 \n",
      "best loss =  783899.402138 \n",
      "last loss =  783899.402138\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 12  ntrees =  13 \n",
      "best loss =  781863.74003 \n",
      "last loss =  781863.74003\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 13  ntrees =  14 \n",
      "best loss =  779840.273575 \n",
      "last loss =  779840.273575\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 14  ntrees =  15 \n",
      "best loss =  777836.59199 \n",
      "last loss =  777836.59199\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 15  ntrees =  16 \n",
      "best loss =  775850.981438 \n",
      "last loss =  775850.981438\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 16  ntrees =  17 \n",
      "best loss =  773855.752847 \n",
      "last loss =  773855.752847\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 17  ntrees =  18 \n",
      "best loss =  771881.012677 \n",
      "last loss =  771881.012677\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 18  ntrees =  19 \n",
      "best loss =  769918.103175 \n",
      "last loss =  769918.103175\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 19  ntrees =  20 \n",
      "best loss =  767971.306276 \n",
      "last loss =  767971.306276\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 20  ntrees =  21 \n",
      "best loss =  766031.438015 \n",
      "last loss =  766031.438015\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 21  ntrees =  22 \n",
      "best loss =  764107.608338 \n",
      "last loss =  764107.608338\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 22  ntrees =  23 \n",
      "best loss =  762188.818365 \n",
      "last loss =  762188.818365\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 23  ntrees =  24 \n",
      "best loss =  760273.641095 \n",
      "last loss =  760273.641095\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 24  ntrees =  25 \n",
      "best loss =  758384.142163 \n",
      "last loss =  758384.142163\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 25  ntrees =  26 \n",
      "best loss =  756518.518652 \n",
      "last loss =  756518.518652\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 26  ntrees =  27 \n",
      "best loss =  754645.183218 \n",
      "last loss =  754645.183218\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 27  ntrees =  28 \n",
      "best loss =  752775.67915 \n",
      "last loss =  752775.67915\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 28  ntrees =  29 \n",
      "best loss =  750927.838396 \n",
      "last loss =  750927.838396\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 29  ntrees =  30 \n",
      "best loss =  749096.115845 \n",
      "last loss =  749096.115845\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 30  ntrees =  31 \n",
      "best loss =  747256.972297 \n",
      "last loss =  747256.972297\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 31  ntrees =  32 \n",
      "best loss =  745443.253237 \n",
      "last loss =  745443.253237\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 32  ntrees =  33 \n",
      "best loss =  743623.010304 \n",
      "last loss =  743623.010304\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 33  ntrees =  34 \n",
      "best loss =  741813.672445 \n",
      "last loss =  741813.672445\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 34  ntrees =  35 \n",
      "best loss =  740030.832504 \n",
      "last loss =  740030.832504\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 35  ntrees =  36 \n",
      "best loss =  738252.254168 \n",
      "last loss =  738252.254168\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 36  ntrees =  37 \n",
      "best loss =  736483.285099 \n",
      "last loss =  736483.285099\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 37  ntrees =  38 \n",
      "best loss =  734732.98627 \n",
      "last loss =  734732.98627\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 38  ntrees =  39 \n",
      "best loss =  732984.088685 \n",
      "last loss =  732984.088685\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 39  ntrees =  40 \n",
      "best loss =  731245.953672 \n",
      "last loss =  731245.953672\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 40  ntrees =  41 \n",
      "best loss =  729511.198112 \n",
      "last loss =  729511.198112\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 41  ntrees =  42 \n",
      "best loss =  727784.272864 \n",
      "last loss =  727784.272864\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 42  ntrees =  43 \n",
      "best loss =  726064.392053 \n",
      "last loss =  726064.392053\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 43  ntrees =  44 \n",
      "best loss =  724363.051558 \n",
      "last loss =  724363.051558\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 44  ntrees =  45 \n",
      "best loss =  722675.537154 \n",
      "last loss =  722675.537154\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 45  ntrees =  46 \n",
      "best loss =  721003.493455 \n",
      "last loss =  721003.493455\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 46  ntrees =  47 \n",
      "best loss =  719332.081855 \n",
      "last loss =  719332.081855\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 47  ntrees =  48 \n",
      "best loss =  717673.107972 \n",
      "last loss =  717673.107972\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 48  ntrees =  49 \n",
      "best loss =  716012.561654 \n",
      "last loss =  716012.561654\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 49  ntrees =  50 \n",
      "best loss =  714378.616763 \n",
      "last loss =  714378.616763\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 50  ntrees =  51 \n",
      "best loss =  712746.066127 \n",
      "last loss =  712746.066127\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 51  ntrees =  52 \n",
      "best loss =  711121.430395 \n",
      "last loss =  711121.430395\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 52  ntrees =  53 \n",
      "best loss =  709503.996771 \n",
      "last loss =  709503.996771\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 53  ntrees =  54 \n",
      "best loss =  707907.419043 \n",
      "last loss =  707907.419043\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 54  ntrees =  55 \n",
      "best loss =  706319.936393 \n",
      "last loss =  706319.936393\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 55  ntrees =  56 \n",
      "best loss =  704733.85331 \n",
      "last loss =  704733.85331\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 56  ntrees =  57 \n",
      "best loss =  703165.158862 \n",
      "last loss =  703165.158862\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 57  ntrees =  58 \n",
      "best loss =  701600.419091 \n",
      "last loss =  701600.419091\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 58  ntrees =  59 \n",
      "best loss =  700038.747678 \n",
      "last loss =  700038.747678\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 59  ntrees =  60 \n",
      "best loss =  698498.489849 \n",
      "last loss =  698498.489849\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 60  ntrees =  61 \n",
      "best loss =  696961.225484 \n",
      "last loss =  696961.225484\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 61  ntrees =  62 \n",
      "best loss =  695436.683357 \n",
      "last loss =  695436.683357\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 62  ntrees =  63 \n",
      "best loss =  693916.699207 \n",
      "last loss =  693916.699207\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 63  ntrees =  64 \n",
      "best loss =  692406.227991 \n",
      "last loss =  692406.227991\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 64  ntrees =  65 \n",
      "best loss =  690898.289813 \n",
      "last loss =  690898.289813\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 65  ntrees =  66 \n",
      "best loss =  689404.466269 \n",
      "last loss =  689404.466269\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 66  ntrees =  67 \n",
      "best loss =  687921.201888 \n",
      "last loss =  687921.201888\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 67  ntrees =  68 \n",
      "best loss =  686446.177271 \n",
      "last loss =  686446.177271\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 68  ntrees =  69 \n",
      "best loss =  684973.061604 \n",
      "last loss =  684973.061604\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 69  ntrees =  70 \n",
      "best loss =  683525.123102 \n",
      "last loss =  683525.123102\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 70  ntrees =  71 \n",
      "best loss =  682066.06915 \n",
      "last loss =  682066.06915\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 71  ntrees =  72 \n",
      "best loss =  680623.958409 \n",
      "last loss =  680623.958409\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 72  ntrees =  73 \n",
      "best loss =  679190.386476 \n",
      "last loss =  679190.386476\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 73  ntrees =  74 \n",
      "best loss =  677759.630017 \n",
      "last loss =  677759.630017\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 74  ntrees =  75 \n",
      "best loss =  676340.718084 \n",
      "last loss =  676340.718084\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 75  ntrees =  76 \n",
      "best loss =  674931.438047 \n",
      "last loss =  674931.438047\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 76  ntrees =  77 \n",
      "best loss =  673522.536694 \n",
      "last loss =  673522.536694\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 77  ntrees =  78 \n",
      "best loss =  672132.966409 \n",
      "last loss =  672132.966409\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 78  ntrees =  79 \n",
      "best loss =  670746.624535 \n",
      "last loss =  670746.624535\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 79  ntrees =  80 \n",
      "best loss =  669359.599239 \n",
      "last loss =  669359.599239\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 80  ntrees =  81 \n",
      "best loss =  667995.784197 \n",
      "last loss =  667995.784197\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 81  ntrees =  82 \n",
      "best loss =  666636.451217 \n",
      "last loss =  666636.451217\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 82  ntrees =  83 \n",
      "best loss =  665284.970371 \n",
      "last loss =  665284.970371\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 83  ntrees =  84 \n",
      "best loss =  663942.046135 \n",
      "last loss =  663942.046135\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 84  ntrees =  85 \n",
      "best loss =  662609.311995 \n",
      "last loss =  662609.311995\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 85  ntrees =  86 \n",
      "best loss =  661278.58466 \n",
      "last loss =  661278.58466\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 86  ntrees =  87 \n",
      "best loss =  659955.857943 \n",
      "last loss =  659955.857943\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 87  ntrees =  88 \n",
      "best loss =  658643.549118 \n",
      "last loss =  658643.549118\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 88  ntrees =  89 \n",
      "best loss =  657335.812085 \n",
      "last loss =  657335.812085\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 89  ntrees =  90 \n",
      "best loss =  656038.752927 \n",
      "last loss =  656038.752927\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 90  ntrees =  91 \n",
      "best loss =  654750.851665 \n",
      "last loss =  654750.851665\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 91  ntrees =  92 \n",
      "best loss =  653482.230777 \n",
      "last loss =  653482.230777\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 92  ntrees =  93 \n",
      "best loss =  652211.291635 \n",
      "last loss =  652211.291635\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 93  ntrees =  94 \n",
      "best loss =  650941.114697 \n",
      "last loss =  650941.114697\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 94  ntrees =  95 \n",
      "best loss =  649680.624958 \n",
      "last loss =  649680.624958\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 95  ntrees =  96 \n",
      "best loss =  648421.079564 \n",
      "last loss =  648421.079564\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 96  ntrees =  97 \n",
      "best loss =  647163.833621 \n",
      "last loss =  647163.833621\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 97  ntrees =  98 \n",
      "best loss =  645930.022835 \n",
      "last loss =  645930.022835\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 98  ntrees =  99 \n",
      "best loss =  644702.587192 \n",
      "last loss =  644702.587192\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 99  ntrees =  100 \n",
      "best loss =  643475.947005 \n",
      "last loss =  643475.947005\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 100  ntrees =  101 \n",
      "best loss =  642264.363532 \n",
      "last loss =  642264.363532\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 101  ntrees =  102 \n",
      "best loss =  641043.065429 \n",
      "last loss =  641043.065429\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 102  ntrees =  103 \n",
      "best loss =  639837.887686 \n",
      "last loss =  639837.887686\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 103  ntrees =  104 \n",
      "best loss =  638647.005632 \n",
      "last loss =  638647.005632\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 104  ntrees =  105 \n",
      "best loss =  637455.632628 \n",
      "last loss =  637455.632628\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 105  ntrees =  106 \n",
      "best loss =  636262.572071 \n",
      "last loss =  636262.572071\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 106  ntrees =  107 \n",
      "best loss =  635086.092443 \n",
      "last loss =  635086.092443\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 107  ntrees =  108 \n",
      "best loss =  633915.69598 \n",
      "last loss =  633915.69598\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 108  ntrees =  109 \n",
      "best loss =  632760.736204 \n",
      "last loss =  632760.736204\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 109  ntrees =  110 \n",
      "best loss =  631609.107058 \n",
      "last loss =  631609.107058\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 110  ntrees =  111 \n",
      "best loss =  630451.071476 \n",
      "last loss =  630451.071476\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 111  ntrees =  112 \n",
      "best loss =  629313.361703 \n",
      "last loss =  629313.361703\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 112  ntrees =  113 \n",
      "best loss =  628182.467949 \n",
      "last loss =  628182.467949\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 113  ntrees =  114 \n",
      "best loss =  627054.24995 \n",
      "last loss =  627054.24995\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 114  ntrees =  115 \n",
      "best loss =  625940.87945 \n",
      "last loss =  625940.87945\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 115  ntrees =  116 \n",
      "best loss =  624824.990776 \n",
      "last loss =  624824.990776\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 116  ntrees =  117 \n",
      "best loss =  623721.617228 \n",
      "last loss =  623721.617228\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 117  ntrees =  118 \n",
      "best loss =  622618.764503 \n",
      "last loss =  622618.764503\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 118  ntrees =  119 \n",
      "best loss =  621520.220191 \n",
      "last loss =  621520.220191\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 119  ntrees =  120 \n",
      "best loss =  620430.257358 \n",
      "last loss =  620430.257358\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 120  ntrees =  121 \n",
      "best loss =  619342.632724 \n",
      "last loss =  619342.632724\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 121  ntrees =  122 \n",
      "best loss =  618266.705906 \n",
      "last loss =  618266.705906\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 122  ntrees =  123 \n",
      "best loss =  617192.852329 \n",
      "last loss =  617192.852329\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 123  ntrees =  124 \n",
      "best loss =  616117.415219 \n",
      "last loss =  616117.415219\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 124  ntrees =  125 \n",
      "best loss =  615060.175759 \n",
      "last loss =  615060.175759\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 125  ntrees =  126 \n",
      "best loss =  614013.573379 \n",
      "last loss =  614013.573379\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 126  ntrees =  127 \n",
      "best loss =  612970.488742 \n",
      "last loss =  612970.488742\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 127  ntrees =  128 \n",
      "best loss =  611927.927734 \n",
      "last loss =  611927.927734\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 128  ntrees =  129 \n",
      "best loss =  610895.215061 \n",
      "last loss =  610895.215061\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 129  ntrees =  130 \n",
      "best loss =  609868.87119 \n",
      "last loss =  609868.87119\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 130  ntrees =  131 \n",
      "best loss =  608850.045583 \n",
      "last loss =  608850.045583\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 131  ntrees =  132 \n",
      "best loss =  607836.069826 \n",
      "last loss =  607836.069826\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 132  ntrees =  133 \n",
      "best loss =  606835.588912 \n",
      "last loss =  606835.588912\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 133  ntrees =  134 \n",
      "best loss =  605833.344638 \n",
      "last loss =  605833.344638\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 134  ntrees =  135 \n",
      "best loss =  604831.877158 \n",
      "last loss =  604831.877158\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 135  ntrees =  136 \n",
      "best loss =  603837.047546 \n",
      "last loss =  603837.047546\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 136  ntrees =  137 \n",
      "best loss =  602851.041707 \n",
      "last loss =  602851.041707\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 137  ntrees =  138 \n",
      "best loss =  601872.240379 \n",
      "last loss =  601872.240379\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 138  ntrees =  139 \n",
      "best loss =  600893.96555 \n",
      "last loss =  600893.96555\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 139  ntrees =  140 \n",
      "best loss =  599916.470979 \n",
      "last loss =  599916.470979\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 140  ntrees =  141 \n",
      "best loss =  598950.082933 \n",
      "last loss =  598950.082933\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 141  ntrees =  142 \n",
      "best loss =  597993.791879 \n",
      "last loss =  597993.791879\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 142  ntrees =  143 \n",
      "best loss =  597038.542381 \n",
      "last loss =  597038.542381\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 143  ntrees =  144 \n",
      "best loss =  596091.197873 \n",
      "last loss =  596091.197873\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 144  ntrees =  145 \n",
      "best loss =  595151.694948 \n",
      "last loss =  595151.694948\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 145  ntrees =  146 \n",
      "best loss =  594215.052167 \n",
      "last loss =  594215.052167\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 146  ntrees =  147 \n",
      "best loss =  593281.238476 \n",
      "last loss =  593281.238476\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 147  ntrees =  148 \n",
      "best loss =  592353.478363 \n",
      "last loss =  592353.478363\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 148  ntrees =  149 \n",
      "best loss =  591421.942363 \n",
      "last loss =  591421.942363\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 149  ntrees =  150 \n",
      "best loss =  590495.987194 \n",
      "last loss =  590495.987194\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 150  ntrees =  151 \n",
      "best loss =  589588.420857 \n",
      "last loss =  589588.420857\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 151  ntrees =  152 \n",
      "best loss =  588679.821188 \n",
      "last loss =  588679.821188\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 152  ntrees =  153 \n",
      "best loss =  587781.071047 \n",
      "last loss =  587781.071047\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 153  ntrees =  154 \n",
      "best loss =  586886.062189 \n",
      "last loss =  586886.062189\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 154  ntrees =  155 \n",
      "best loss =  585990.994327 \n",
      "last loss =  585990.994327\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 155  ntrees =  156 \n",
      "best loss =  585105.201596 \n",
      "last loss =  585105.201596\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 156  ntrees =  157 \n",
      "best loss =  584223.182895 \n",
      "last loss =  584223.182895\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 157  ntrees =  158 \n",
      "best loss =  583352.278293 \n",
      "last loss =  583352.278293\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 158  ntrees =  159 \n",
      "best loss =  582481.845316 \n",
      "last loss =  582481.845316\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 159  ntrees =  160 \n",
      "best loss =  581619.089944 \n",
      "last loss =  581619.089944\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 160  ntrees =  161 \n",
      "best loss =  580763.275532 \n",
      "last loss =  580763.275532\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 161  ntrees =  162 \n",
      "best loss =  579906.903977 \n",
      "last loss =  579906.903977\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 162  ntrees =  163 \n",
      "best loss =  579053.580933 \n",
      "last loss =  579053.580933\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 163  ntrees =  164 \n",
      "best loss =  578198.989127 \n",
      "last loss =  578198.989127\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 164  ntrees =  165 \n",
      "best loss =  577353.223445 \n",
      "last loss =  577353.223445\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 165  ntrees =  166 \n",
      "best loss =  576508.528979 \n",
      "last loss =  576508.528979\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 166  ntrees =  167 \n",
      "best loss =  575668.596259 \n",
      "last loss =  575668.596259\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 167  ntrees =  168 \n",
      "best loss =  574837.185636 \n",
      "last loss =  574837.185636\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 168  ntrees =  169 \n",
      "best loss =  574016.008462 \n",
      "last loss =  574016.008462\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 169  ntrees =  170 \n",
      "best loss =  573200.320354 \n",
      "last loss =  573200.320354\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 170  ntrees =  171 \n",
      "best loss =  572379.496121 \n",
      "last loss =  572379.496121\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 171  ntrees =  172 \n",
      "best loss =  571577.544448 \n",
      "last loss =  571577.544448\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 172  ntrees =  173 \n",
      "best loss =  570769.631315 \n",
      "last loss =  570769.631315\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 173  ntrees =  174 \n",
      "best loss =  569975.589752 \n",
      "last loss =  569975.589752\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 174  ntrees =  175 \n",
      "best loss =  569184.937566 \n",
      "last loss =  569184.937566\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 175  ntrees =  176 \n",
      "best loss =  568396.659156 \n",
      "last loss =  568396.659156\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 176  ntrees =  177 \n",
      "best loss =  567609.759346 \n",
      "last loss =  567609.759346\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 177  ntrees =  178 \n",
      "best loss =  566833.411083 \n",
      "last loss =  566833.411083\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 178  ntrees =  179 \n",
      "best loss =  566052.848669 \n",
      "last loss =  566052.848669\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 179  ntrees =  180 \n",
      "best loss =  565272.995334 \n",
      "last loss =  565272.995334\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 180  ntrees =  181 \n",
      "best loss =  564497.990272 \n",
      "last loss =  564497.990272\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 181  ntrees =  182 \n",
      "best loss =  563735.388277 \n",
      "last loss =  563735.388277\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 182  ntrees =  183 \n",
      "best loss =  562976.999317 \n",
      "last loss =  562976.999317\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 183  ntrees =  184 \n",
      "best loss =  562221.040557 \n",
      "last loss =  562221.040557\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 184  ntrees =  185 \n",
      "best loss =  561471.399171 \n",
      "last loss =  561471.399171\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 185  ntrees =  186 \n",
      "best loss =  560726.943631 \n",
      "last loss =  560726.943631\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 186  ntrees =  187 \n",
      "best loss =  559980.32982 \n",
      "last loss =  559980.32982\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 187  ntrees =  188 \n",
      "best loss =  559243.64876 \n",
      "last loss =  559243.64876\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 188  ntrees =  189 \n",
      "best loss =  558511.379319 \n",
      "last loss =  558511.379319\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 189  ntrees =  190 \n",
      "best loss =  557787.310249 \n",
      "last loss =  557787.310249\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 190  ntrees =  191 \n",
      "best loss =  557063.229511 \n",
      "last loss =  557063.229511\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 191  ntrees =  192 \n",
      "best loss =  556343.173573 \n",
      "last loss =  556343.173573\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 192  ntrees =  193 \n",
      "best loss =  555630.847806 \n",
      "last loss =  555630.847806\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 193  ntrees =  194 \n",
      "best loss =  554919.116451 \n",
      "last loss =  554919.116451\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 194  ntrees =  195 \n",
      "best loss =  554206.594008 \n",
      "last loss =  554206.594008\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 195  ntrees =  196 \n",
      "best loss =  553497.119003 \n",
      "last loss =  553497.119003\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 196  ntrees =  197 \n",
      "best loss =  552798.026778 \n",
      "last loss =  552798.026778\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 197  ntrees =  198 \n",
      "best loss =  552101.886038 \n",
      "last loss =  552101.886038\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 198  ntrees =  199 \n",
      "best loss =  551410.284304 \n",
      "last loss =  551410.284304\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 199  ntrees =  200 \n",
      "best loss =  550714.269838 \n",
      "last loss =  550714.269838\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 200  ntrees =  201 \n",
      "best loss =  550032.368635 \n",
      "last loss =  550032.368635\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 201  ntrees =  202 \n",
      "best loss =  549356.707439 \n",
      "last loss =  549356.707439\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 202  ntrees =  203 \n",
      "best loss =  548675.45135 \n",
      "last loss =  548675.45135\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 203  ntrees =  204 \n",
      "best loss =  548003.934661 \n",
      "last loss =  548003.934661\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 204  ntrees =  205 \n",
      "best loss =  547334.450704 \n",
      "last loss =  547334.450704\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 205  ntrees =  206 \n",
      "best loss =  546669.833511 \n",
      "last loss =  546669.833511\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 206  ntrees =  207 \n",
      "best loss =  546011.645087 \n",
      "last loss =  546011.645087\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 207  ntrees =  208 \n",
      "best loss =  545354.051347 \n",
      "last loss =  545354.051347\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 208  ntrees =  209 \n",
      "best loss =  544698.45306 \n",
      "last loss =  544698.45306\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 209  ntrees =  210 \n",
      "best loss =  544050.924989 \n",
      "last loss =  544050.924989\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 210  ntrees =  211 \n",
      "best loss =  543402.666113 \n",
      "last loss =  543402.666113\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 211  ntrees =  212 \n",
      "best loss =  542760.112628 \n",
      "last loss =  542760.112628\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 212  ntrees =  213 \n",
      "best loss =  542117.635194 \n",
      "last loss =  542117.635194\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 213  ntrees =  214 \n",
      "best loss =  541475.375698 \n",
      "last loss =  541475.375698\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 214  ntrees =  215 \n",
      "best loss =  540843.962309 \n",
      "last loss =  540843.962309\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 215  ntrees =  216 \n",
      "best loss =  540211.516781 \n",
      "last loss =  540211.516781\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 216  ntrees =  217 \n",
      "best loss =  539587.228899 \n",
      "last loss =  539587.228899\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 217  ntrees =  218 \n",
      "best loss =  538964.377643 \n",
      "last loss =  538964.377643\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 218  ntrees =  219 \n",
      "best loss =  538347.628255 \n",
      "last loss =  538347.628255\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 219  ntrees =  220 \n",
      "best loss =  537733.950202 \n",
      "last loss =  537733.950202\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 220  ntrees =  221 \n",
      "best loss =  537125.678385 \n",
      "last loss =  537125.678385\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 221  ntrees =  222 \n",
      "best loss =  536519.29115 \n",
      "last loss =  536519.29115\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 222  ntrees =  223 \n",
      "best loss =  535915.642442 \n",
      "last loss =  535915.642442\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 223  ntrees =  224 \n",
      "best loss =  535319.123757 \n",
      "last loss =  535319.123757\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 224  ntrees =  225 \n",
      "best loss =  534723.149509 \n",
      "last loss =  534723.149509\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 225  ntrees =  226 \n",
      "best loss =  534128.018178 \n",
      "last loss =  534128.018178\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 226  ntrees =  227 \n",
      "best loss =  533531.893662 \n",
      "last loss =  533531.893662\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 227  ntrees =  228 \n",
      "best loss =  532943.676483 \n",
      "last loss =  532943.676483\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 228  ntrees =  229 \n",
      "best loss =  532354.452259 \n",
      "last loss =  532354.452259\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 229  ntrees =  230 \n",
      "best loss =  531770.310461 \n",
      "last loss =  531770.310461\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 230  ntrees =  231 \n",
      "best loss =  531193.243873 \n",
      "last loss =  531193.243873\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 231  ntrees =  232 \n",
      "best loss =  530620.239193 \n",
      "last loss =  530620.239193\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 232  ntrees =  233 \n",
      "best loss =  530050.408452 \n",
      "last loss =  530050.408452\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 233  ntrees =  234 \n",
      "best loss =  529483.28081 \n",
      "last loss =  529483.28081\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 234  ntrees =  235 \n",
      "best loss =  528919.690698 \n",
      "last loss =  528919.690698\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 235  ntrees =  236 \n",
      "best loss =  528352.614014 \n",
      "last loss =  528352.614014\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 236  ntrees =  237 \n",
      "best loss =  527787.652715 \n",
      "last loss =  527787.652715\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 237  ntrees =  238 \n",
      "best loss =  527233.606638 \n",
      "last loss =  527233.606638\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 238  ntrees =  239 \n",
      "best loss =  526682.921105 \n",
      "last loss =  526682.921105\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 239  ntrees =  240 \n",
      "best loss =  526136.455264 \n",
      "last loss =  526136.455264\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 240  ntrees =  241 \n",
      "best loss =  525592.27248 \n",
      "last loss =  525592.27248\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 241  ntrees =  242 \n",
      "best loss =  525050.969681 \n",
      "last loss =  525050.969681\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 242  ntrees =  243 \n",
      "best loss =  524514.220719 \n",
      "last loss =  524514.220719\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 243  ntrees =  244 \n",
      "best loss =  523978.385676 \n",
      "last loss =  523978.385676\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 244  ntrees =  245 \n",
      "best loss =  523445.49098 \n",
      "last loss =  523445.49098\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 245  ntrees =  246 \n",
      "best loss =  522915.707668 \n",
      "last loss =  522915.707668\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 246  ntrees =  247 \n",
      "best loss =  522388.441817 \n",
      "last loss =  522388.441817\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 247  ntrees =  248 \n",
      "best loss =  521864.550483 \n",
      "last loss =  521864.550483\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 248  ntrees =  249 \n",
      "best loss =  521336.372601 \n",
      "last loss =  521336.372601\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 249  ntrees =  250 \n",
      "best loss =  520816.394485 \n",
      "last loss =  520816.394485\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 250  ntrees =  251 \n",
      "best loss =  520295.522334 \n",
      "last loss =  520295.522334\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 251  ntrees =  252 \n",
      "best loss =  519777.301753 \n",
      "last loss =  519777.301753\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 252  ntrees =  253 \n",
      "best loss =  519269.453011 \n",
      "last loss =  519269.453011\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 253  ntrees =  254 \n",
      "best loss =  518759.365433 \n",
      "last loss =  518759.365433\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 254  ntrees =  255 \n",
      "best loss =  518254.866246 \n",
      "last loss =  518254.866246\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 255  ntrees =  256 \n",
      "best loss =  517755.123159 \n",
      "last loss =  517755.123159\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 256  ntrees =  257 \n",
      "best loss =  517253.637108 \n",
      "last loss =  517253.637108\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 257  ntrees =  258 \n",
      "best loss =  516756.894754 \n",
      "last loss =  516756.894754\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 258  ntrees =  259 \n",
      "best loss =  516266.898751 \n",
      "last loss =  516266.898751\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 259  ntrees =  260 \n",
      "best loss =  515773.942412 \n",
      "last loss =  515773.942412\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 260  ntrees =  261 \n",
      "best loss =  515287.072022 \n",
      "last loss =  515287.072022\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 261  ntrees =  262 \n",
      "best loss =  514801.102461 \n",
      "last loss =  514801.102461\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 262  ntrees =  263 \n",
      "best loss =  514321.719893 \n",
      "last loss =  514321.719893\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 263  ntrees =  264 \n",
      "best loss =  513843.578222 \n",
      "last loss =  513843.578222\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 264  ntrees =  265 \n",
      "best loss =  513368.331027 \n",
      "last loss =  513368.331027\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 265  ntrees =  266 \n",
      "best loss =  512894.987906 \n",
      "last loss =  512894.987906\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 266  ntrees =  267 \n",
      "best loss =  512425.702267 \n",
      "last loss =  512425.702267\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n",
      "\n",
      "iteration # 267  ntrees =  268 \n",
      "best loss =  511958.527237 \n",
      "last loss =  511958.527237\n",
      "learning_rate =  0.003\n",
      "sample_size 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-20f074b5a0a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'res_greedy = greedy.greed_up_features_bfs(trees,trainFactory,\\n                                          loss = MSELoss,\\n                                          learning_rate = .003,\\n                                          learning_rate_decay=1.,# no decay\\n                                          nTrees =600,\\n                                          trees_sample_size =500, #chosen from the ensemble at random each iteration\\n                                          verbose = True,\\n                                          regularizer=0.0004, #added to gradient walker\\'s leaf denominator\\n                                          use_joblib=True,\\n                                          n_jobs=-1,\\n                                          joblib_method=\"threads\" #every GIL-ly thing is copied anyways\\n                                          )'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2264\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2265\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/root/notebook/ezhik/pruner/pruning/greedy.pyc\u001b[0m in \u001b[0;36mgreed_up_features_bfs\u001b[1;34m(trees, factory, loss, learning_rate, breadth, nTrees, trees_sample_size, verbose, learning_rate_decay, trees_sample_increase, regularizer, use_joblib, n_jobs, joblib_method, copy_pred, initialBunch)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 _res = joblib.Parallel(n_jobs = n_jobs,\n\u001b[1;32m--> 150\u001b[1;33m                                backend = \"threading\")(tasks)\n\u001b[0m\u001b[0;32m    151\u001b[0m                 \u001b[0m_additions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib64/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    664\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib64/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib64/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib64/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/rh/python27/root/usr/lib64/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout, balancing)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_greedy = greedy.greed_up_features_bfs(trees,trainFactory,\n",
    "                                          loss = MSELoss,\n",
    "                                          learning_rate = .003,\n",
    "                                          learning_rate_decay=1.,# no decay\n",
    "                                          nTrees =600,\n",
    "                                          trees_sample_size =500, #chosen from the ensemble at random each iteration\n",
    "                                          verbose = True,\n",
    "                                          regularizer=0.0004, #added to gradient walker's leaf denominator\n",
    "                                          use_joblib=True,\n",
    "                                          n_jobs=-1,\n",
    "                                          joblib_method=\"threads\" #every GIL-ly thing is copied anyways\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_stupid = greedy.predict(testFactory,trees[:600])\n",
    "y_pred_full = greedy.predict(testFactory,trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_greedy = testFactory.predict(res_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metrics.mean_squared_error(Yts,y_pred_greedy),\n",
    "print metrics.mean_squared_error(Yts,y_pred_stupid),\n",
    "print metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usability distribution\n",
    "thresholds = mnet.get_thresholds(trees,formula.feature_ids,0.001,use_joblib = True,b=n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(thresholds)),thresholds[:,2])\n",
    "print sum(thresholds[:,2] >150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get them...\n",
    "thresholds_active = thresholds[thresholds[:,2]>100] #at least 100 times used in the original ensemble\n",
    "print len(thresholds_active)\n",
    "criteria = hierarchy.select_criteria(trainFactory,thresholds_active,4,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split = hierarchy.split_upper(trainFactory,criteria,equalizeWeights=False,split_weights=1.,split_inclusion=.7) \n",
    "#при каждом разделении в подвыборку  попадает split_inclusion примеров из другой половины выборки с весом split_weights\n",
    "#print [split[i].events.shape[0] for i in split]\n",
    "#print [sum(split[i].weights) for i in split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hierarchical stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#note that it equalizes weights which might be suboptimal.\n",
    "trees_splitted = hierarchy.train_splitted_boosts(trees, trainFactory,criteria,\n",
    "                                                 breadth = 1,\n",
    "                                                 loss = MSELoss,\n",
    "                                                 learning_rate = 0.005, \n",
    "                                                 nTrees_leaf= 600,\n",
    "                                                 trees_sample_size=500,\n",
    "                                                 regularizer =0.0004,\n",
    "                                                 verbose=True,use_joblib = True,n_jobs = -1,\n",
    "                                                 weights_outside_leaf = 0.75**.25, inclusion_outside_leaf = 0.**.25) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_splitted= hierarchy.predict_splitted(testFactory,criteria,trees_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_test = testFactory.weights\n",
    "Yts = testFactory.labels\n",
    "print 'spltd\\t',metrics.mean_squared_error(Yts,y_pred_splitted)\n",
    "print 'greedy\\t',metrics.mean_squared_error(Yts,y_pred_greedy)\n",
    "print 'stupid\\t',metrics.mean_squared_error(Yts,y_pred_stupid)\n",
    "print 'full\\t',metrics.mean_squared_error(Yts,y_pred_full)\n",
    "print \"well...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MSE learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_curve(formula,factory, metric,split_criteria = None):\n",
    "    \n",
    "    splitted = split_criteria is not None\n",
    "    lcurve = []\n",
    "\n",
    "    Ypred = np.zeros(len(factory.labels))\n",
    "    lcurve.append(metric(factory.labels, Ypred,sample_weight = factory.weights))\n",
    "                  \n",
    "    for tree in formula:\n",
    "        if splitted:\n",
    "            trees_i = {code:formula[code][i] for code in formula}\n",
    "            tree_pred = hierarchy.predict_splitted(factory,split_criteria,trees_i)\n",
    "        else:\n",
    "            tree_pred = factory.predict([tree])\n",
    "\n",
    "        Ypred += tree_pred\n",
    "        \n",
    "        lcurve.append(metric(factory.labels, Ypred,sample_weight = factory.weights))\n",
    "        \n",
    "    return lcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric_name = 'MSE'\n",
    "n_trees = 600\n",
    "metric = metrics.mean_squared_error\n",
    "\n",
    "stupid_lcurve = learning_curve(trees[:600],testFactory,metric)\n",
    "greedy_lcurve = learning_curve(res_greedy[:600],testFactory,metric)\n",
    "splitted_lcurve = learning_curve(res_greedy[:600],testFactory,metric,criteria)\n",
    "\n",
    "p = range(1,n_trees)\n",
    "plt.figure(figsize = [14,14])\n",
    "plt.plot(p,[0.935834322801 for i in range(1,n_trees)],label = \"full\")\n",
    "plt.plot(p,stupid_lcurve[1:n_trees],label = \"stupid\")\n",
    "plt.plot(p,greedy_lcurve[1:n_trees],label = \"greedy\")\n",
    "plt.plot(p,splitted_lcurve[1:n_trees],label = \"splitted\")\n",
    "plt.title('learning curves('+metric_name+')')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
